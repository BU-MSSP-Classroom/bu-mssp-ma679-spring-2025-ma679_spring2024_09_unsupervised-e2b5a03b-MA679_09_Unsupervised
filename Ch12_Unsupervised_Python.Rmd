---
title: "Unsupervised Model"
author: "Your Name"
date: "2024-2-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message=FALSE,fig.align="center",fig.width=7,fig.height=7)

```

```{css,echo=FALSE}
.btn {
    border-width: 0 0px 0px 0px;
    font-weight: normal;
    text-transform: ;
}

.btn-default {
    color: #2ecc71;
    background-color: #ffffff;
    border-color: #ffffff;
}
```

```{r,echo=FALSE}
# Global parameter
show_code <- TRUE
```

# Class Workbook {.tabset .tabset-fade .tabset-pills}

## In class activity
```{python}
import numpy as np
import pandas as pd
#import math
from matplotlib.pyplot import subplots
#import statsmodels.api as sm
from plotnine import *
import plotly.express as px
#import plotly.express as px
#import statsmodels.formula.api as sm
#import ISLP as islp
from sklearn import datasets
from sklearn.model_selection import train_test_split
import seaborn as sns
from matplotlib import pyplot as plt
from matplotlib.lines import Line2D
from sklearn.datasets import make_blobs 
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE


import geopandas as gpd
```

### Fraud Detection

The data is synthetic datasets generated by the PaySim mobile money simulator
https://www.kaggle.com/datasets/ealaxi/paysim1


```{python,echo=show_code}
import zipfile
Fd= pd.read_csv('fraud.csv.gz',compression='gzip')
```

PaySim simulates mobile money transactions based on a sample of actual transactions extracted from one month of financial logs from a mobile money service implemented in an African country. The original logs were provided by a multinational company, the mobile financial service provider, which is currently running in more than 14 countries worldwide.  This synthetic dataset is scaled down 1/4 of the original dataset.

Here are the variables.

- step: 1 step is 1 hour. Total steps 744 (30 days simulation).
- type: CASH-IN, CASH-OUT, DEBIT, PAYMENT, and TRANSFER.
- amount: the amount of the transaction in local currency.
- nameOrig: the customer who started the transaction
- oldbalanceOrg: initial balance before the transaction
- newbalanceOrig: new balance after the transaction
- nameDest: the customer who is the recipient of the transaction
- oldbalanceDest: initial balance recipient before the transaction. Note that there is no information for customers that start with M (Merchants).
- newbalanceDest: new balance recipient after the transaction. Note that there is no information for customers that start with M (Merchants).
- isFraud: This is the transactions made by the fraudulent agents inside the simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control of customers accounts and try to empty the funds by transferring them to another account and then cashing out of the system.
- isFlaggedFraud: The business model aims to control massive transfers from one account to another and flags illegal attempts. An illegal attempt in this dataset is to transfer more than 200.000 in a single transaction.

The goal of this exercise is to find ways to identify if the transaction is fraudulent.  However, pretend that you are not given `isFraud` label since, in many cases, you don't know at the time of the transaction.  You can assume you have access to `isFlaggedFraud` label at the time of your project.  Try to use the method discussed in the chapter to identify fraudulent activities.


#### Pokemon Types

Pokemon is a popular game that's been around for ages.  This data set from [Kaggle](https://www.kaggle.com/datasets/abcsds/pokemon) includes 800 Pokemon, including their number, name, first and second type, and basic stats: HP, Attack, Defense, Special Attack, Special Defense, and Speed. 

(ID)#: ID for each pokemon
Name: Name of each pokemon
Type 1: Each pokemon has a type, this determines weakness/resistance to attacks
Type 2: Some pokemon are dual type and have 2
Total: sum of all stats that come after this, a general guide to how strong a pokemon is
HP: hit points, or health, defines how much damage a pokemon can withstand before fainting
Attack: the base modifier for normal attacks (eg. Scratch, Punch)
Defense: the base damage resistance against normal attacks
SP Atk: special attack, the base modifier for special attacks (e.g. fire blast, bubble beam)
SP Def: the base damage resistance against special attacks
Speed: determines which pokemon attacks first each round

```{python}
Pokemon = pd.read_csv('Pokemon.csv')
```

Do EDA and find patterns in the data. Notice that this data is multivariate, with some clear dependency in the variables. Does the result make sense?  

```{python}
#
#
```

Comment of the result:

~~~
Please write your answer in full sentences.


~~~

Your task is to group the Pokemon into some meaningful clusters that makes sense to you and have a discussion with your neighbor on why your clusters make the most sense. How many cluster seems appropriate?

```{python}
#
#
```

Comment of the result:

~~~
Please write your answer in full sentences.


~~~

Look at your cluster and see if you can identify a monster that does not seem to fit with the other monsters. Why do you think that might be?  If you want to know more about these Pokemons you can do a deeper dive by looking through the [database](https://pokemondb.net/pokedex/all).

```{python}
#
#
```

Comment of the result:

~~~
Please write your answer in full sentences.


~~~

### Image cluster

Let's look at Fashion MNIST data.
https://www.kaggle.com/datasets/zalando-research/fashionmnist

Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), representing the clothing article. The rest of the columns contain the pixel-values of the associated image.

```{python}
fm=pd.read_csv('fashion-mnist_train_sub.csv')
```

- Each row in the data is a separate image.
- Remaining columns are pixel numbers (784 total).  To locate a pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27. The pixel is located on row i and column j of a 28 x 28 matrix. For example, pixel31 indicates the pixel in the fourth column from the left, and the second row from the top.
- Each value is the darkness of the pixel (1 to 255)
- Column 1 is the class label ( which we will assume is not given )
  0 T-shirt/top
  1 Trouser
  2 Pullover
  3 Dress
  4 Coat
  5 Sandal
  6 Shirt
  7 Sneaker
  8 Bag
  9 Ankle boot
  

```{python}
xrange = np.arange(0, 28, 1)
yrange = np.arange(0, 28, 1)
xx, yy = np.meshgrid(xrange, yrange)
gmat = pd.DataFrame({'x':xx.ravel(),'y':yy.ravel()})

# df1.reset_index(drop=True, inplace=True)
# df2.reset_index(drop=True, inplace=True)
# 
# df = pd.concat([df1, df2], axis=1) 
tempimg=fm.iloc[3,1:785]
tempimg.reset_index(drop=True, inplace=True)
ptdf=pd.concat([gmat["x"],gmat["y"],tempimg], names=['x','y', 'value'], axis=1)
ptdf.columns =["x","y","value"]
(
ggplot(ptdf) + 
  geom_raster(aes(x = "x", y = "y", fill = "value")) + 
  scale_fill_gradient(low = "white", high = "black") + 
  theme(aspect_ratio = 1, legend_position = "none") + 
  labs(x = "", y = "") + 
  scale_y_reverse( expand = (0, 0)) 
)

```

## Problem Set


### Euclidian distance and Correlation

In the chapter, we mentioned the use of correlation-based distance and Euclidean distance as dissimilarity measures for hierarchical clustering. It turns out that these two measures are almost equivalent: if each observation has been centered to have mean zero and standard deviation one, and if we let $r_{ij}$ denote the correlation between the ith and jth observations, then the quantity $1-r_{ij}$ is proportional to the squared Euclidean distance between the ith and jth observations. On the `USArrests` data, show that this proportionality holds. 

Hint: The Euclidean distance can be calculated using the `pairwise_distances()` function from the `sklearn.metrics` module, and correlations can be calculated using the `np.corrcoef()` function.

### Percent Variance Explained

A formula for calculating PVE was given as
$$\frac{\sum_i^n z_{im}^2}{\sum_j^p\sum_i^n x_{ij}^2}=\frac{\sum_i^n (\sum_j^p\phi_{jm}x_{ij}    )^2}{\sum_j^p\sum_i^n x_{ij}^2}$$
We also saw that the PVE can be obtained using the `explained_variance_ratio_` attribute of a fitted `PCA()` estimator.
On the `USArrests` data, calculate PVE in two ways:

(a) Using the `explained_variance_ratio_` output of the fitted `PCA()` estimator, as was done in Section 12.2.3.
(b) By applying Equation 12.10 directly. The loadings are stored as the `components_` attribute of the fitted `PCA()` estimator. Use those loadings in Equation 12.10 to obtain the PVE.

Hint: You will only obtain the same results in (a) and (b) if the same
data is used in both cases. For instance, if in (a) you performed PCA()
using centered and scaled variables, then you must center and scale
the variables before applying Equation 12.10 in (b).

### Hierarchical clustering of USA Arrest

Consider the `USArrests` data. We will now perform hierarchical clustering on the states.
(a) Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.

Your code:
```{python,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

(b) Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?

Your code:
```{python,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

(c) Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one.

Your code:
```{python,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

(d) What effect does scaling the variables have on the hierarchical clustering obtained? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed? Provide a justification for your answer.

Your code:
```{python,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

### Simulation PCA and Kmeans

In this problem, you will generate simulated data, and then perform PCA and K-means clustering on the data.

(a) Generate a simulated data set with 20 observations in each of
three classes (i.e. 60 observations total), and 50 variables.
Hint: There are a number of functions in Python that you can
use to generate data. One example is the normal() method of
the random() function in numpy; the uniform() method is another
option. Be sure to add a mean shift to the observations in each
class so that there are three distinct classes.
Your code:
```{python,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

(b) Perform PCA on the 60 observations and plot the first two principal
component score vectors. Use a different color to indicate
the observations in each of the three classes. If the three classes
appear separated in this plot, then continue on to part (c). If
not, then return to part (a) and modify the simulation so that
there is greater separation between the three classes. Do not
continue to part (c) until the three classes show at least some
separation in the first two principal component score vectors.

Your code:
```{python,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

(c) Perform K-means clustering of the observations with K = 3.
How well do the clusters that you obtained in K-means clustering
compare to the true class labels?
Hint: You can use the pd.crosstab() function in Python to compare
the true class labels to the class labels obtained by clustering.
Be careful how you interpret the results: K-means clustering
will arbitrarily number the clusters, so you cannot simply check
whether the true class labels and clustering labels are the same.
Your code:
```{python,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~


(d) Perform K-means clustering with K = 2. Describe your results.
Your code:
```{python,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

(e) Now perform K-means clustering with K = 4, and describe your results.

Your code:
```{python,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

(f) Now perform K-means clustering with K = 3 on the first two
principal component score vectors, rather than on the raw data.
That is, perform K-means clustering on the 60 × 2 matrix of
which the first column is the first principal component score
vector, and the second column is the second principal component
score vector. Comment on the results.

Your code:
```{python,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

(g) Using the StandardScaler() estimator, perform K-means clustering
with K = 3 on the data after scaling each variable to have
standard deviation one. How do these results compare to those
obtained in (b)? Explain.

Your code:
```{python,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

### Matrix completion

Write a Python function to perform matrix completion as in Algorithm
12.1, and as outlined in Section 12.5.2. In each iteration, the
function should keep track of the relative error, as well as the iteration
count. Iterations should continue until the relative error is small
enough or until some maximum number of iterations is reached (set a
default value for this maximum number). Furthermore, there should
be an option to print out the progress in each iteration.
Test your function on the Boston data. First, standardize the features
to have mean zero and standard deviation one using the
`StandardScaler()` function. Run an experiment where you randomly
leave out an increasing (and nested) number of observations from 5%
to 30%, in steps of 5%. Apply Algorithm 12.1 with $M = 1, 2,\dots, 8$.
Display the approximation error as a function of the fraction of observations
that are missing, and the value of M, averaged over 10
repetitions of the experiment.

Your code:
```{python,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

### Matrix completion


In Section 12.5.2, Algorithm 12.1 was implemented using the
svd() function from the np.linalg module. However, given the connection
between the svd() function and the PCA() estimator highlighted
in the lab, we could have instead implemented the algorithm
using PCA().
Write a function to implement Algorithm 12.1 that makes use of PCA()
rather than svd().

Your code:
```{python,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~


### Gene expression

The dataset on gene expression (Ch12Ex13.csv) consists of 40 tissue samples with measurements on 1,000 genes. The first 20 samples are from healthy patients, while the second 20 are from a diseased group.
```{python}
credit_data=pd.read_csv("Ch12Ex13.csv")
```

(a) Load in the data using pd.read_csv(). You will need to select `header = None`.

Your code:
```{python,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~


(b) Apply hierarchical clustering to the samples using correlationbased
distance, and plot the dendrogram. Do the genes separate
the samples into the two groups? Do your results depend on the
type of linkage used?

Your code:
```{python,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~


(c) Your collaborator wants to know which genes differ the most
across the two groups. Suggest a way to answer this question,
and apply it here.

Your code:
```{python,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~




## Additional Material


### pheatmap

pheatmap in R is popular in bioinformatics because it combines hierarchical clustering, heatmap, and k-means at once.  You can row cluster and column cluster at the same time. In python the closest you can find is in seaborn. https://seaborn.pydata.org/generated/seaborn.clustermap.html

```{python,fig.width=7,fig.height=7.5}
iris = datasets.load_iris()
iris_pd = pd.DataFrame(data= np.c_[iris['data'], iris['target']],
                     columns= ["sepal_length","sepal_width","petal_length","petal_width"] + ['Species'])
iris_pd["sample_id"]=    list(range(1,151)  )   
iris_pd = iris_pd.set_index('sample_id')

my_palette = dict(zip(iris_pd["Species"].unique(), ["orange","yellow","brown"]))
row_colors = iris_pd["Species"].map(my_palette)
tmp=sns.clustermap(iris_pd,  method="ward", cmap="Blues", standard_scale=1, row_colors=row_colors)
```

### visualizing K-means result

There is nice visualization for k-means clustering.

```{python,fig.width=7,fig.height=7.5}

feature, target = make_blobs(n_samples=600, 
                             centers=3, 
                             random_state=123, 
                             shuffle=True) 
                            
kmeansfit = KMeans(n_clusters=3, random_state=0, n_init="auto").fit(feature)

df = pd.DataFrame({'x1': feature[:, 0], 'x2': feature[:, 1], 'target': target})
plt.scatter(df["x1"], df["x2"], alpha = 0.6, s=10)
plt.show()
df['cluster'] = kmeansfit.fit_predict(feature)# get centroids
centroids = kmeansfit.cluster_centers_
cen_x = [i[0] for i in centroids] 
cen_y = [i[1] for i in centroids]
## add to df
df['cen_x'] = df.cluster.map({0:cen_x[0], 1:cen_x[1], 2:cen_x[2]})
df['cen_y'] = df.cluster.map({0:cen_y[0], 1:cen_y[1], 2:cen_y[2]})# define and map colors
colors = ['#DF2020', '#81DF20', '#2095DF']
df['c'] = df.cluster.map({0:colors[0], 1:colors[1], 2:colors[2]})

fig, ax = plt.subplots(1, figsize=(8,8))
# plot data
plt.scatter(df["x1"], df["x2"], c=df.c, alpha = 0.3, s=10)

# plot centroids
plt.scatter(cen_x, cen_y, marker='*', c=colors, s=70)# plot Attack mean
plt.show()
# 
# plt.plot([df.x1.mean()]*2, [-2,2], color='black', lw=0.5, linestyle='--')
# plt.xlim(-2,2)# plot Defense mean
# plt.plot([-2,2], [df.x2.mean()]*2, color='black', lw=0.5, linestyle='--')
# plt.ylim(-2,2)# create a list of legend elemntes
# ## average line
# legend_elements = [Line2D([0], [0], color='black', lw=0.5, linestyle='--', label='Average')]
# ## markers / records
# cluster_leg = [Line2D([0], [0], marker='o', color='w', label='Cluster {}'.format(i+1), 
#                markerfacecolor=mcolor, markersize=5) for i, mcolor in enumerate(colors)]
# ## centroids
# cent_leg = [Line2D([0], [0], marker='^', color='w', label='Centroid - C{}'.format(i+1), 
#             markerfacecolor=mcolor, markersize=10) for i, mcolor in enumerate(colors)]
# # add all elements to the same list
# legend_elements.extend(cluster_leg)
# legend_elements.extend(cent_leg)
# # plot legend
# plt.legend(handles=legend_elements, loc='upper right', ncol=2)# title and labels
# #plt.title('Pokemon Stats\n', loc='left', fontsize=22)
# plt.xlabel('x1')
# plt.ylabel('x2')
# plt.show()
```



Another nice way to visualize K-means results when there are not too many k is the radar chart.
Returning to the USArrest example, we can use k-means and then inspect whether the clusters make sense.

```{python,fig.width=7,fig.height=7.5}
std_scaler = StandardScaler()

USArrests = pd.read_csv('USArrests.csv')#
USArrests=USArrests.rename(columns={"Unnamed: 0":"State"})
USArrests_kmeansfit = KMeans(n_clusters=5, random_state=0, n_init="auto").fit(std_scaler.fit_transform(USArrests.loc[:,["Murder","Assault",  "UrbanPop",  "Rape"]]))

df = pd.DataFrame(USArrests_kmeansfit.cluster_centers_,columns=["Murder","Assault",  "UrbanPop",  "Rape"])
df["Cluster"]=list(range(1,6))
USArrests["Cluster"]=USArrests_kmeansfit.fit_predict(USArrests.loc[:,["Murder","Assault",  "UrbanPop",  "Rape"]])
# USA_cluster_mean=USArrests.loc[:,["Murder","Assault",  "UrbanPop",  "Rape","Cluster"]].groupby("Cluster").mean(numeric_only=True)
# USA_cluster_mean["Cluster"]=USA_cluster_mean.index
USA_x_m=pd.melt(df, id_vars=['Cluster'], 
          value_vars=["Murder","Assault",  "UrbanPop",  "Rape"])
fig = px.line_polar(USA_x_m, r = 'value', theta = 'variable', line_close = True,
                    color = 'Cluster')
#fig.update_traces(fill = 'toself')

fig.show()


states = gpd.read_file('cb_2018_us_state_500k')

non_continental = ['HI','VI','MP','GU','AK','AS','PR']
us49 = states
for n in non_continental:
    us49 = us49[us49.STUSPS != n]
states=us49
# Assuming km.usA$cluster is your clustering result
# Replace the following line with your clustering result
km_usA_cluster = {'cluster': [0, 1, 2, 3], 'region': ['alabama', 'alaska', 'arizona', 'arkansas']}  # Sample data

# Convert km.usA$cluster to a DataFrame
dfft = pd.DataFrame(USArrests)

# Convert region names to lowercase to match with states data
dfft['region'] = dfft['State'].str.lower()

colors=sns.color_palette("husl",5).as_hex()

# Merge the states GeoDataFrame with the clustering results DataFrame
statec = states.merge(dfft, left_on='NAME', right_on='State', how='right')
statec['c'] = statec.Cluster.map({0:colors[0], 1:colors[1], 2:colors[2],3:colors[3],4:colors[4]})

fig, ax = plt.subplots(figsize=(10, 10))
fig.subplots_adjust(hspace=0.0, wspace=0.0)
statec.plot("Cluster", color=statec.c,legend=True,ax=ax)
xmin, ymin, xmax, ymax = states.total_bounds
pad = 5  # add a padding around the geometry
ax.set_xlim(xmin-pad, xmax+pad)
ax.set_ylim(ymin-pad, ymax+pad)
plt.show()


```



### Choice of the distance and linkage

Weird things happen for a particular choice of linkage methods.

This is an example where using the centroid flips.

```{python ,fig.width=8,fig.height=3}
x=[5.24,4.58,6.25,6.15,5.02,5.95]
y=[3.16,2.78,2.24,4.04,3.99,2.77]
data = list(zip(x, y))

print(data)
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering
fig, (ax0, ax1, ax2) = plt.subplots(1, 3)
ax0.scatter(x,y)
linkage_data = linkage(data, method='single', metric='euclidean')
dendrogram(linkage_data,ax=ax1)
linkage_data = linkage(data, method='centroid', metric='euclidean')
dendrogram(linkage_data,ax=ax2)
plt.show()

```

The different combinations will return different results.  Knowing that you cannot be ignorant about the method is essential.

### Other distances

There are many different distances and some are good for binary variables.
You can find most of them in `scipy.spatial.distance`
https://docs.scipy.org/doc/scipy/reference/spatial.distance.html#module-scipy.spatial.distance

- euclidean	$$d_{jk} = \sqrt{(\sum(x_{ij}-x_{ik})^2)}$$
	binary:$sqrt(A+B-2*J)$
- manhattan	$$d_{jk} = \sum(|x_{ij} - x_{ik}|)$$
	binary:$A+B-2*J$
- gower	$$d_{jk}  = \frac{1}{M} \sum\left(\frac{|x_{ij}-x_{ik}|}{(\max(x_{i})-\min(x_{i}))}\right)$$
	binary:$(A+B-2*J)/M$,
	where M is the number of columns (excluding missing values)
- altGower	$$d_{jk}  = ( \frac{1}{NZ} ) \sum(|x_{ij} - x_{ik}|)$$
	where $NZ$ is the number of non-zero columns excluding double-zeros (Anderson et al. 2006).
	binary:$(A+B-2*J)/(A+B-J)$
- canberra	$$d_{jk} = \frac{1}{NZ}  \sum \left(\frac{(x_{ij}-x_{ik})}{(x_{ij}+x_{ik})}\right)$$
	whereNZis the number of non-zero entries.
	binary:$(A+B-2*J)/(A+B-J)$
- bray	$$d_{jk}  = \left(\sum |x_{ij}-x_{ik}| \right)/\left(\sum (x_{ij}+x_{ik})\right)$$
	binary:$(A+B-2*J)/(A+B)$
- kulczynski	$$d_{jk} = 1 - 0.5*((\sum \min(x_{ij},x_{ik})/(\sum x_{ij}) + (\sum min(x_{ij},x_{ik})/(\sum x_{ik}))$$
	binary:$1-(J/A + J/B)/2$
- morisita	$$d_{jk} = 1 - 2*\frac{\sum\left(x_{ij}*x_{ik}\right)}{((\lambda_{j}+\lambda_{k}) * \sum(x_{ij})*\sum(x_{ik}))}$$, where
	$\lambda_{j} = \sum(x_{ij}*(x_{ij}-1))/\sum(x_{ij})*\sum(x_{ij}-1)$
	
	binary: cannot be calculated
	
- horn	$$d_{jk} = 1 - 2*\frac{\sum\left(x_{ij}*x_{ik}\right)}{((\lambda_{j}+\lambda_{k}) * \sum(x_{ij})*\sum(x_{ik}))}$$ ( Same as morisita ),
 but $$\lambda_{j} = \sum(x_{ij}^2)/(\sum(x[ij])^2)$$
	binary:$(A+B-2*J)/(A+B)$
- binomial	$$d_{jk} = \sum\left(x_{ij}*\log(\frac{x_{ij}}{n_{i}}) + x_{ik}*\log(\frac{x_{ik}}{n_{i}}) - n_{i}*\log( \frac{1}{2} )\right)/n_{i}$$,
	where $n_{i} = x_{ij} + x_{ik}$
	
	binary:$\log(2)*(A+B-2*J)$
	
- cao	$$d_{jk}  = \frac{1}{S} * sum(log(n_{i}/2) - (x_{ij}*\log(x_{ik}) + x_{ik}*\log(x_{ij}))/n_{i})$$,
	where $S$ is the number of species in compared sites and $n_{i} = x_{ij} + x_{ik}$

You can find these in the vegan package.
```{python}
pdist_Methods=['braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation', 'cosine', 'dice', 'euclidean', 'hamming', 'jaccard', 'jensenshannon', 'kulczynski1', 'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']

xx = np.transpose(np.random.binomial(n=1, p=0.5, size=(1000, 100)))

from sklearn.metrics.pairwise import pairwise_distances
pairwise_distances(xx,metric="jaccard")

from scipy.spatial.distance import pdist
pdist(xx, 'euclidean')
```

### Cluster performance indices

There are various indices proposed for comparing the clustering result. 
Few of the popular ones are implemented in `sklearn.metrics`
|----------------------------------|------------------------------------------------------
| ‘adjusted_mutual_info_score’     | metrics.adjusted_mutual_info_score
| ‘adjusted_rand_score’            | metrics.adjusted_rand_score
| ‘completeness_score’             | metrics.completeness_score
| ‘fowlkes_mallows_score’          | metrics.fowlkes_mallows_score
| ‘homogeneity_score’              | metrics.homogeneity_score
| ‘mutual_info_score’              | metrics.mutual_info_score
| ‘normalized_mutual_info_score’   | metrics.normalized_mutual_info_score
| ‘rand_score’                     | metrics.rand_score
| ‘v_measure_score’                | metrics.v_measure_score

Here is synthetic data with 6 clusters.

```{python,fig.width=7,fig.height=7.5}
from sklearn.datasets import make_blobs 
feature, target = make_blobs(n_samples=600, 
                             centers=6, 
                             random_state=123, 
                             shuffle=True) 
mydata = pd.concat([pd.DataFrame(feature, columns=['x1', 'x2']), pd.DataFrame(target, columns=['y'])], axis=1)
plt.scatter(mydata["x1"], mydata["x2"]) 
plt.show()
```

If we manually calculate the WGSS

```{python,eval=TRUE}

from sklearn.cluster import KMeans

# Assuming `mydata` is a numpy array or pandas DataFrame containing your data
wss=np.zeros(15)
wss[0] = (mydata.shape[0] - 1) * np.sum(np.apply_along_axis(np.var, 0, mydata))

for i in range(2, 16):
    kmeans = KMeans(n_clusters=i, random_state=0, n_init="auto")
    kmeans.fit(mydata)
    wss[i - 1] = kmeans.inertia_

plt.plot(range(1, 16), wss, marker='o', linestyle='-')
plt.xlabel('Number of Clusters')
plt.ylabel('Within groups sum of squares')
plt.title('Elbow Method')
plt.show()
```

The WGSS stops decreasing around K=5 to 6, which is correct for this problem.

```{python}

kmeans = KMeans(n_clusters=6, random_state=0, n_init="auto")
kmeans.fit(mydata)
#mydata["cluster"]= kmeans.labels_
plt.scatter(mydata["x1"], mydata["x2"],c=kmeans.labels_) 
plt.show()
```

#### [silhouette score](https://en.wikipedia.org/wiki/Silhouette_(clustering))

Silhouette Scores are also popular metric to look at in clustering problem. Score near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster. 

https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html


```{python}
from sklearn.metrics import silhouette_samples, silhouette_score
silhouette_avg = silhouette_score(mydata, kmeans.labels_)
sample_silhouette_values = silhouette_samples(mydata, kmeans.labels_)

ses=np.zeros(14)
for i in range(2, 16):
    kmeans = KMeans(n_clusters=i, random_state=0, n_init="auto")
    kmeans.fit(mydata)
    ses[i - 2] = silhouette_score(mydata, kmeans.labels_)
    

plt.plot(range(1, 15), ses, marker='o', linestyle='-')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette score')
plt.title('Silhouette Analysis')
plt.show()
```

```{python,echo=FALSE}

range_n_clusters = [2, 3, 4, 5, 6, 7]

n_clusters=6
# Create a subplot with 1 row and 2 columns
fig, (ax1, ax2) = plt.subplots(1, 2)
fig.set_size_inches(18, 7)

# The 1st subplot is the silhouette plot
# The silhouette coefficient can range from -1, 1 but in this example all
# lie within [-0.1, 1]
ax1.set_xlim([-0.1, 1])
# The (n_clusters+1)*10 is for inserting blank space between silhouette
# plots of individual clusters, to demarcate them clearly.
ax1.set_ylim([0, len(mydata) + (n_clusters + 1) * 10])

# Initialize the clusterer with n_clusters value and a random generator
# seed of 10 for reproducibility.
clusterer = KMeans(n_clusters=n_clusters, random_state=10)
cluster_labels = clusterer.fit_predict(mydata)

# The silhouette_score gives the average value for all the samples.
# This gives a perspective into the density and separation of the formed
# clusters
silhouette_avg = silhouette_score(mydata, cluster_labels)
print(
    "For n_clusters =",
    n_clusters,
    "The average silhouette_score is :",
    silhouette_avg,
)

# Compute the silhouette scores for each sample
sample_silhouette_values = silhouette_samples(mydata, cluster_labels)

y_lower = 10


for i in range(n_clusters):
    # Aggregate the silhouette scores for samples belonging to
    # cluster i, and sort them
    ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]
    
    ith_cluster_silhouette_values.sort()
    import matplotlib.cm as cm
    size_cluster_i = ith_cluster_silhouette_values.shape[0]
    y_upper = y_lower + size_cluster_i
    
    color = cm.nipy_spectral(float(i) / n_clusters)
    ax1.fill_betweenx(
        np.arange(y_lower, y_upper),
        0,
        ith_cluster_silhouette_values,
        facecolor=color,
        edgecolor=color,
        alpha=0.7,
    )
    
    # Label the silhouette plots with their cluster numbers at the middle
    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
    
    # Compute the new y_lower for next plot
    y_lower = y_upper + 10  # 10 for the 0 samples


ax1.set_title("The silhouette plot for the various clusters.")
ax1.set_xlabel("The silhouette coefficient values")
ax1.set_ylabel("Cluster label")

# The vertical line for average silhouette score of all the values
ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

ax1.set_yticks([])  # Clear the yaxis labels / ticks
ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

# 2nd Plot showing the actual clusters formed
colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
ax2.scatter(
    mydata.loc[:, "x1"],mydata.loc[:, "x2"], marker=".", s=30, lw=0, alpha=0.7, c=colors, edgecolor="k"
)

# Labeling the clusters
centers = clusterer.cluster_centers_
# Draw white circles at cluster centers
ax2.scatter(
    centers[:, 0],
    centers[:, 1],
    marker="o",
    c="white",
    alpha=1,
    s=200,
    edgecolor="k",
)

for i, c in enumerate(centers):
    ax2.scatter(c[0], c[1], marker="$%d$" % i, alpha=1, s=50, edgecolor="k")

ax2.set_title("The visualization of the clustered data.")
ax2.set_xlabel("Feature space for the 1st feature")
ax2.set_ylabel("Feature space for the 2nd feature")

plt.suptitle(
    "Silhouette analysis for KMeans clustering on sample data with n_clusters = %d"
    % n_clusters,
    fontsize=14,
    fontweight="bold",
)

plt.show()
```

### Partitioning Around Medoids
K-Medoids or Partitioning (clustering) of the data into k clusters “Around Medoids” (PAM) is a more robust version of K-means.

```{python}
#pip install scikit-learn-extra
from sklearn_extra.cluster import KMedoids

kmedoids = KMedoids(n_clusters=3, random_state=0).fit(iris.data)
kmedoids.labels_
kmedoids.cluster_centers_
kmedcrsstab = pd.crosstab(index=iris.target, 
                           columns=kmedoids.labels_)

```
### Mclust

Mclust is an R package with a collection of model based clustering methods.  mclustpy is the python rapper of the R package.

```{python}
# pip install rpy2
# pip install mclustpy
from mclustpy import mclustpy
mc = mclustpy(iris.data, G=9, modelNames='EEE', random_seed=2020)
mc['classification']

```

### Unsupervised Learning for Timeseries

Working with timeseries data in Unsupervised Learning situations poses several unique challenges. 
The naive way of treating observations along time as a set of features works well only when the timeseries' are well aligned.
For many realistic situations, this is not necessarily the case. Even in a situation where the comparison is about the aligned timeseries, there are problems such as missing data and misalignment of the starting times that need treatment.
https://github.com/aapatel09/handson-unsupervised-learning/blob/master/13_temporal_clustering.ipynb
To make the point, here is CO2 emission by country.  
```{python,echo=FALSE,eval=TRUE}
# pip install openpyxl
#https://github.com/opencasestudies/ocs-bp-co2-emissions/tree/master/data/raw
CO2_emissions = pd.read_excel("yearly_co2_emissions_1000_tonnes.xlsx")
CO2_emissions_subset=CO2_emissions[CO2_emissions.country.isin(["United States","Canada","China","United Kingdom","France","India"])]
memm=pd.melt(CO2_emissions_subset, id_vars=['country'])
#as.integer(as.character(variable))
(
  ggplot(memm)+geom_point()+geom_line()+
  aes(x="variable",y="value",color="country")+
  scale_y_log10()+xlab("year")+ylab("Co2 Emission") + 
  theme(axis_text_x = element_text(angle = 45, hjust = 1, vjust = 1))
)
```
Let's say you want to cluster the countries by their trend.  The time is aligned since each country has the same years.
However, you can see that the measurement goes back much longer in time for UK than other countries. Also there are gaps in the measurements that need to be taken care of.  One way to deal with such data is to limit your observations to years that are more stable.

```{python,eval=TRUE}
memm2000=memm[memm.variable>2000]
(
  ggplot(memm2000)+
  geom_point()+geom_line()+
  aes(x="variable",y="value",color="country",group="country")+
  scale_y_log10()+xlab("year")+ylab("Co2 Emission")
)
```

Another example is to determine the letter being written from the trajectories of a writer. Below are lines for five examples of the X velocity for A and B.
https://archive.ics.uci.edu/dataset/175/character+trajectories

```{python,eval=T}
CharTraj=pd.read_csv("CharTraj.csv")
plt.plot(range(0,203), CharTraj.iloc[0,2:205]) 
plt.plot(range(0,203), CharTraj.iloc[1,2:205]) 
plt.plot(range(0,203), CharTraj.iloc[2,2:205]) 
plt.plot(range(0,203), CharTraj.iloc[3,2:205]) 
plt.plot(range(0,203), CharTraj.iloc[4,2:205]) 
plt.show()
plt.plot(range(0,203), CharTraj.iloc[5,2:205]) 
plt.plot(range(0,203), CharTraj.iloc[6,2:205]) 
plt.plot(range(0,203), CharTraj.iloc[7,2:205]) 
plt.plot(range(0,203), CharTraj.iloc[8,2:205]) 
plt.plot(range(0,203), CharTraj.iloc[9,2:205]) 
plt.show()
# import scipy.io
# CharTraj = scipy.io.loadmat('character_trajectories/mixoutALL_shifted.mat')
# from matplotlib.lines import Line2D
# plt.plot(range(0,178), CharTraj.get("mixout")[0][1][1,:]) 
# Line2D(range(0,178), CharTraj.get("mixout")[0][2][1,:]) 
# Line2D(range(0,178), CharTraj.get("mixout")[0][3][1,:]) 
# Line2D(range(0,178), CharTraj.get("mixout")[0][4][1,:]) 
# Line2D(range(0,178), CharTraj.get("mixout")[0][6][1,:]) 
# plt.show()
# 
# plt.plot(range(0,130), CharTraj.get("mixout")[0][174][1,:]) 
# plt.plot(range(0,130), CharTraj.get("mixout")[0][175][1,:]) 
# plt.plot(range(0,130), CharTraj.get("mixout")[0][176][1,:]) 
# plt.plot(range(0,130), CharTraj.get("mixout")[0][177][1,:]) 
# plt.plot(range(0,130), CharTraj.get("mixout")[0][178][1,:]) 
# plt.show()
# 
# (CharTraj.get("consts")[0][0][3])
# pd.DataFrame(CharTraj.get("consts")[0][0][4][0]).value_counts().sort_index()
```

This will be a whole timeseries comparison within fixed time window, but you have a misalignment in time that would distort point-by-point distance.

Suppose the goal is to extract particular activity from measurements taken on a wearable device and to classify what is being done. In that case, an additional segmentation issue must be considered.

#### Dynamic Time Warping (DTW)

Dynamic Time warping is a way to calculate distances between datapoints when the datapoints are shifted horizontally in time between each other, but the shape is consistent.  Unlike Euclidean distance, for DTW, the two time series do not have to have an equal length. DTW calculates the smallest distance between all points and then finds the closest matching points.  Here is an illustration of how it works. (https://rpubs.com/esobolewska/dtw-time-series)

```{python,echo=FALSE,eval=TRUE}

# pip install dtw-python # you will need to install
from dtw import dtw as dtwpy_dtw
from dtw import *
#pip install dtaidistance # you will need to install
from dtaidistance import dtw as dtad_dtw
#pip install tslearn # you will need to install
# from tslearn.metrics import dtw as ts_dtw

## A noisy sine wave as query
idx = np.linspace(0,6.28,num=100)
query = np.sin(idx) + np.random.uniform(size=100)/10.0

## A cosine is for template; sin and cos are offset by 25 samples
template = np.cos(idx)

a1 = query# np.array([7, 9, 6, 9, 12, 6, 4, 6, 8])
a2 = template#np.array([5, 6, 4, 3, 9, 5, 6, 8, 9])

# Calculate Dynamic Time Warping (DTW)
dtad_distance = dtad_dtw.distance(a1, a2)
#ts_distance   = ts_dtw(a1, a2) # problem with reticulate
#
ptwpy_distance = dtwpy_dtw(a1, a2, keep_internals=True)

## Display the warping curve, i.e. the alignment curve
ptwpy_distance.plot(type="threeway")

# Plotting
xrange = [np.min(a1),np.max(a1)+1]
yrange = [np.min(a2), np.max(a2)+ 1]

plt.plot(xrange, yrange, color='white')  # Set up the plot
plt.plot(a1, color='blue', linestyle='-')  # Plot a1
plt.plot(a2, color='magenta', linestyle='-')  # Plot a2
plt.xlabel('time')
plt.ylabel('value')
plt.show()
```

```{python}
dtw(a1, a2, keep_internals=True, 
    step_pattern=rabinerJuangStepPattern(6, "c"))\
    .plot(type="twoway",offset=-2)
plt.show()
## See the recursion relation, as formula and diagram
print(rabinerJuangStepPattern(6,"c"))
rabinerJuangStepPattern(6,"c").plot()
```

using pyts library
```{python,eval=FALSE}
#pip install pyts # you will need to install
from pyts.metrics import dtw as pyts_dtw

pyts_distance = pyts_dtw(a1, a2) # problem with reticulate
n_timestamps_1, n_timestamps_2 = a1.size,a2.size

plt.figure(figsize=(10, 8))
timestamps_1 = np.arange(n_timestamps_1 + 1)
timestamps_2 = np.arange(n_timestamps_2 + 1)

# Dynamic Time Warping: classic
dtw_classic, path_classic = pyts_dtw(a1, a2, dist='square',
                                method='classic', return_path=True)
matrix_classic = np.zeros((n_timestamps_1, n_timestamps_2))
matrix_classic[tuple(path_classic)] = 1.

plt.subplot(2, 2, 1)
plt.pcolor(timestamps_1, timestamps_2, matrix_classic.T,
           edgecolors='k', cmap='Greys')
plt.xlabel('x', fontsize=12)
plt.ylabel('y', fontsize=12)
plt.title("{0}\nDTW(x, y) = {1:.2f}".format('classic', dtw_classic),
          fontsize=14)
# 
# plot(dtwfit, xlab="a1 - blue", ylab="a2 - magenta", 
#      xaxp  = c(0,10,10), yaxp = c(0,10,10), type="threeway")
# plot(dtwfit, xaxp  = c(0,10,10), yaxp = c(0,10,10), type="twoway", col=c('blue', 'magenta'))
```

tslearn.clustering has few useful functions.
But for some reason it does not work with Rstudio
```{python,eval=FALSE}
from tslearn.clustering import TimeSeriesKMeans
from tslearn.clustering import KShape
scaler = StandardScaler()
co2emissions_norm=pd.DataFrame(scaler.fit_transform(CO2_emissions.iloc[:,254:265].dropna()),columns=CO2_emissions.iloc[:,254:265].columns)

N_CLUSTERS=5
model = TimeSeriesKMeans(n_clusters=N_CLUSTERS, metric="dtw",
                         max_iter=10, random_state=1234)
model.fit(co2emissions_norm)
y_pred = model.predict(co2emissions_norm)
plt.figure()
plt.subplots(figsize = (20, 16))
for yi in range(N_CLUSTERS):
    plt.subplot(10, 6, yi + 1)
    for xx in co2emissions_norm[y_pred == yi]:
        plt.plot(xx.ravel(), "k-", alpha=.2)
    plt.plot(model.cluster_centers_[yi].ravel(), "r-")
    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),
             transform=plt.gca().transAxes)
    if yi == 1:
        plt.title("Euclidean $k$-means")
```

For the hand writing data.
```{python,eval=F}
CharTraj_fill=CharTraj.iloc[:,2:205].fillna(0)
kshape=KShape(n_clusters=20,
                         max_iter=10, random_state=1234)
kshape.fit(CharTraj_fill)
# hc <- tsclust(CharTraj, type = "hierarchical", k = 20L, 
#               distance = "sbd", trace = TRUE,
#               control = hierarchical_control(method = "average"))
# plot(hc)


# tdf<-melt(CharTraj) # %>% group_by(L1) %>% mutate(id = row_number())
# tcf<-melt(hc@cluster)
# tcf$L1<-rownames(tcf)
# tdf<-merge(tdf,tcf,by ="L1")
# tdf$num <- sequence(rle(tdf$L1)$lengths)
# ggplot(tdf)+geom_line(aes(x=num,y=value.x,group=L1))+facet_wrap(~value.y)
```


### DBScan

The DBScan clustering algorithm is as follows:

1. Randomly select a point i.
2. Retrieve all the points that are density reachable from i concerning the Maximum radius of the neighborhood (EPS) and the minimum number of points within eps neighborhood(Min Pts).
3. If the number of points in the neighborhood is more than Min Pts then i is a core point.
4. For i core points, a cluster is formed. If i is not a core point, mark it as a noise/outlier and move to the next point.
5. Continue the process until all the points have been processed.

```{python,fig.width=7,fig.height=7.5}

from sklearn.cluster import DBSCAN

from sklearn import datasets
from sklearn.model_selection import train_test_split

iris = datasets.load_iris()
clustering = DBSCAN(eps=0.45, min_samples=5).fit(iris.data)

# Checking cluster
labels=clustering.labels_
  
# Table
crsstab = pd.crosstab(index=iris.target, 
                           columns=clustering.labels_)
  
# Plotting Cluster

unique_labels = set(labels)
core_samples_mask = np.zeros_like(labels, dtype=bool)
core_samples_mask[clustering.core_sample_indices_] = True

plt.scatter(iris.data[:, 2], iris.data[:, 3])
plt.show()
colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]
for k, col in zip(unique_labels, colors):
    if k == -1:
        # Black used for noise.
        col = [0, 0, 0, 1]
    
    class_member_mask = labels == k
    
    xy = iris.data[class_member_mask & core_samples_mask]
    plt.plot(
        xy[:, 2],
        xy[:, 3],
        "o",
        markerfacecolor=tuple(col),
        markeredgecolor="k",
        markersize=14,
    )
    
    xy = iris.data[class_member_mask & ~core_samples_mask]
    plt.plot(
        xy[:, 2],
        xy[:, 3],
        "o",
        markerfacecolor=tuple(col),
        markeredgecolor="k",
        markersize=6,
    )

#plt.title(f"Estimated number of clusters: {n_clusters_}")
plt.show()
```


### Other dimension reduction techniques

Various other dimension reduction techniques have been popularized over the years.


####  t-distributed stochastic neighbor embedding (t-SNE) 

First is [t-SNE](https://lvdmaaten.github.io/tsne/) by L.J.P. van der Maaten and G.E. Hinton (2008). The algorithm projects the high-dimensional data points into low dimension (2D) by inducing the projected data to have a similar distribution as the original data points by minimizing KL divergence.

```{python,fig.width=7,fig.height=7.5,eval=TRUE}

X_embedded = TSNE(n_components=2, learning_rate='auto',
                   init='random', perplexity=3).fit_transform(fm.iloc[1:200,1:785])
X_embedded.shape


fig = px.scatter(x=X_embedded[:, 0], y=X_embedded[:, 1], color=fm.iloc[1:200,0])
fig.update_layout(
    title="t-SNE visualization of Custom Classification dataset",
    xaxis_title="First t-SNE",
    yaxis_title="Second t-SNE",
)
#fig.show()

```

#### Uniform Manifold Approximation (UMAP)

UMAP is another dimension reduction described by McInnes and Healy (2018) in <arXiv:1802.03426>.

https://umap-learn.readthedocs.io/en/latest/basic_usage.html

Unfortunately this does not work on reticulate.
```{python,eval=FALSE}
#pip install tensorflow
#pip install umap-learn
#import tensorflow
#fm=tensorflow.keras.datasets.fashion_mnist.load_data()
import umap
from sklearn.datasets import load_digits
digits = load_digits()
fig, ax_array = plt.subplots(20, 20)
axes = ax_array.flatten()
for i, ax in enumerate(axes):
    ax.imshow(digits.images[i], cmap='gray_r')


plt.setp(axes, xticks=[], yticks=[], frame_on=False)
plt.tight_layout(h_pad=0.5, w_pad=0.01)
plt.show()

digits_df = pd.DataFrame(digits.data[:,1:11])
digits_df['digit'] = pd.Series(digits.target).map(lambda x: 'Digit {}'.format(x))
sns.pairplot(digits_df, hue='digit', palette='Spectral');

reducer = umap.UMAP(random_state=42)
reducer.fit(digits.data)
embedding = reducer.transform(digits.data)
# Verify that the result of calling transform is
# idenitical to accessing the embedding_ attribute
assert(np.all(embedding == reducer.embedding_))
embedding.shape

plt.scatter(embedding[:, 0], embedding[:, 1], c=digits.target, cmap='Spectral', s=5)
plt.gca().set_aspect('equal', 'datalim')
plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))
plt.title('UMAP projection of the Digits dataset', fontsize=24);
plt.show()
```


### Using H2O for Unsupervised Learning

```{python}
import h2o
h2o.init()

```

```{python}
# Import the birds dataset into H2O:
birds = h2o.import_file("https://s3.amazonaws.com/h2o-public-test-data/smalldata/pca_test/birds.csv")
```

#### PCA

H2O uses the power method to calculate the singular value decomposition of the Gram matrix.
https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/pca.html
```{python}
from h2o.estimators import H2OPrincipalComponentAnalysisEstimator

# Split the dataset into a train and valid set:
train, valid = birds.split_frame(ratios = [.8], seed = 1234)

# Build and train the model:
birds_pca = H2OPrincipalComponentAnalysisEstimator(k = 5,
                                                   use_all_factor_levels = True,
                                                   pca_method = "glrm",
                                                   transform = "standardize",
                                                   impute_missing = True)
birds_pca.train(training_frame = train)

# Generate predictions on a validation set (if necessary):
pred = birds_pca.predict(valid)
```

#### K-means

https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/k-means.html
```{python}
from h2o.estimators import H2OKMeansEstimator
# Build and train the model:

# Import the iris dataset into H2O:
iris = h2o.import_file("http://h2o-public-test-data.s3.amazonaws.com/smalldata/iris/iris_wheader.csv")

# Set the predictors:
predictors = ["sepal_len", "sepal_wid", "petal_len", "petal_wid"]
# Split the dataset into a train and valid set:
train, valid = iris.split_frame(ratios=[.8], seed=1234)


iris_kmeans = H2OKMeansEstimator(k=10,
                                 estimate_k=True,
                                 standardize=False,
                                 seed=1234)
iris_kmeans.train(x=predictors,
                  training_frame=train,
                  validation_frame=valid)

# Eval performance:
perf = iris_kmeans.model_performance()

#  Generate predictions on a validation set (if necessary):
pred = iris_kmeans.predict(valid)
```

#### Shut down H2O
```{python}
h2o.cluster().shutdown(prompt =False)
```
