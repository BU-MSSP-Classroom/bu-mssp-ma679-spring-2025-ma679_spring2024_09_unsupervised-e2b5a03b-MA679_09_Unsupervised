{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c4a9c2",
   "metadata": {},
   "source": [
    "# Class Workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e515df1",
   "metadata": {},
   "source": [
    "## In class activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a20252f",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import math\n",
    "from matplotlib.pyplot import subplots\n",
    "#import statsmodels.api as sm\n",
    "from plotnine import *\n",
    "import plotly.express as px\n",
    "#import plotly.express as px\n",
    "#import statsmodels.formula.api as sm\n",
    "#import ISLP as islp\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05474a45",
   "metadata": {},
   "source": [
    "### Fraud Detection\n",
    "\n",
    "The data is synthetic datasets generated by the PaySim mobile money simulator\n",
    "https://www.kaggle.com/datasets/ealaxi/paysim1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e08510",
   "metadata": {
    "Rmd_chunk_options": "echo=show_code",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "Fd= pd.read_csv('fraud.csv.gz',compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf6697e",
   "metadata": {},
   "source": [
    "PaySim simulates mobile money transactions based on a sample of actual transactions extracted from one month of financial logs from a mobile money service implemented in an African country. The original logs were provided by a multinational company, the mobile financial service provider, which is currently running in more than 14 countries worldwide.  This synthetic dataset is scaled down 1/4 of the original dataset.\n",
    "\n",
    "Here are the variables.\n",
    "\n",
    "- step: 1 step is 1 hour. Total steps 744 (30 days simulation).\n",
    "- type: CASH-IN, CASH-OUT, DEBIT, PAYMENT, and TRANSFER.\n",
    "- amount: the amount of the transaction in local currency.\n",
    "- nameOrig: the customer who started the transaction\n",
    "- oldbalanceOrg: initial balance before the transaction\n",
    "- newbalanceOrig: new balance after the transaction\n",
    "- nameDest: the customer who is the recipient of the transaction\n",
    "- oldbalanceDest: initial balance recipient before the transaction. Note that there is no information for customers that start with M (Merchants).\n",
    "- newbalanceDest: new balance recipient after the transaction. Note that there is no information for customers that start with M (Merchants).\n",
    "- isFraud: This is the transactions made by the fraudulent agents inside the simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control of customers accounts and try to empty the funds by transferring them to another account and then cashing out of the system.\n",
    "- isFlaggedFraud: The business model aims to control massive transfers from one account to another and flags illegal attempts. An illegal attempt in this dataset is to transfer more than 200.000 in a single transaction.\n",
    "\n",
    "The goal of this exercise is to find ways to identify if the transaction is fraudulent.  However, pretend that you are not given `isFraud` label since, in many cases, you don't know at the time of the transaction.  You can assume you have access to `isFlaggedFraud` label at the time of your project.  Try to use the method discussed in the chapter to identify fraudulent activities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c087329d",
   "metadata": {},
   "source": [
    "#### Pokemon Types\n",
    "\n",
    "Pokemon is a popular game that's been around for ages.  This data set from [Kaggle](https://www.kaggle.com/datasets/abcsds/pokemon) includes 800 Pokemon, including their number, name, first and second type, and basic stats: HP, Attack, Defense, Special Attack, Special Defense, and Speed.\n",
    "\n",
    "(ID)#: ID for each pokemon\n",
    "Name: Name of each pokemon\n",
    "Type 1: Each pokemon has a type, this determines weakness/resistance to attacks\n",
    "Type 2: Some pokemon are dual type and have 2\n",
    "Total: sum of all stats that come after this, a general guide to how strong a pokemon is\n",
    "HP: hit points, or health, defines how much damage a pokemon can withstand before fainting\n",
    "Attack: the base modifier for normal attacks (eg. Scratch, Punch)\n",
    "Defense: the base damage resistance against normal attacks\n",
    "SP Atk: special attack, the base modifier for special attacks (e.g. fire blast, bubble beam)\n",
    "SP Def: the base damage resistance against special attacks\n",
    "Speed: determines which pokemon attacks first each round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0650423a",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "Pokemon = pd.read_csv('Pokemon.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeba8e1",
   "metadata": {},
   "source": [
    "Do EDA and find patterns in the data. Notice that this data is multivariate, with some clear dependency in the variables. Does the result make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af604a8f",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664f2656",
   "metadata": {},
   "source": [
    "Comment of the result:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "Your task is to group the Pokemon into some meaningful clusters that makes sense to you and have a discussion with your neighbor on why your clusters make the most sense. How many cluster seems appropriate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c720f8a4",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26a2b69",
   "metadata": {},
   "source": [
    "Comment of the result:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "Look at your cluster and see if you can identify a monster that does not seem to fit with the other monsters. Why do you think that might be?  If you want to know more about these Pokemons you can do a deeper dive by looking through the [database](https://pokemondb.net/pokedex/all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab6c30",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d3603a",
   "metadata": {},
   "source": [
    "Comment of the result:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d734509e",
   "metadata": {},
   "source": [
    "### Image cluster\n",
    "\n",
    "Let's look at Fashion MNIST data.\n",
    "https://www.kaggle.com/datasets/zalando-research/fashionmnist\n",
    "\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), representing the clothing article. The rest of the columns contain the pixel-values of the associated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70403d48",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "fm=pd.read_csv('fashion-mnist_train_sub.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db59b6e0",
   "metadata": {},
   "source": [
    "- Each row in the data is a separate image.\n",
    "- Remaining columns are pixel numbers (784 total).  To locate a pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27. The pixel is located on row i and column j of a 28 x 28 matrix. For example, pixel31 indicates the pixel in the fourth column from the left, and the second row from the top.\n",
    "- Each value is the darkness of the pixel (1 to 255)\n",
    "- Column 1 is the class label ( which we will assume is not given )\n",
    "  0 T-shirt/top\n",
    "  1 Trouser\n",
    "  2 Pullover\n",
    "  3 Dress\n",
    "  4 Coat\n",
    "  5 Sandal\n",
    "  6 Shirt\n",
    "  7 Sneaker\n",
    "  8 Bag\n",
    "  9 Ankle boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b193cb6",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "xrange = np.arange(0, 28, 1)\n",
    "yrange = np.arange(0, 28, 1)\n",
    "xx, yy = np.meshgrid(xrange, yrange)\n",
    "gmat = pd.DataFrame({'x':xx.ravel(),'y':yy.ravel()})\n",
    "\n",
    "# df1.reset_index(drop=True, inplace=True)\n",
    "# df2.reset_index(drop=True, inplace=True)\n",
    "#\n",
    "# df = pd.concat([df1, df2], axis=1)\n",
    "tempimg=fm.iloc[3,1:785]\n",
    "tempimg.reset_index(drop=True, inplace=True)\n",
    "ptdf=pd.concat([gmat[\"x\"],gmat[\"y\"],tempimg], names=['x','y', 'value'], axis=1)\n",
    "ptdf.columns =[\"x\",\"y\",\"value\"]\n",
    "(\n",
    "ggplot(ptdf) +\n",
    "  geom_raster(aes(x = \"x\", y = \"y\", fill = \"value\")) +\n",
    "  scale_fill_gradient(low = \"white\", high = \"black\") +\n",
    "  theme(aspect_ratio = 1, legend_position = \"none\") +\n",
    "  labs(x = \"\", y = \"\") +\n",
    "  scale_y_reverse( expand = (0, 0))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad23a4d",
   "metadata": {},
   "source": [
    "## Problem Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fe2740",
   "metadata": {},
   "source": [
    "### Euclidian distance and Correlation\n",
    "\n",
    "In the chapter, we mentioned the use of correlation-based distance and Euclidean distance as dissimilarity measures for hierarchical clustering. It turns out that these two measures are almost equivalent: if each observation has been centered to have mean zero and standard deviation one, and if we let $r_{ij}$ denote the correlation between the ith and jth observations, then the quantity $1-r_{ij}$ is proportional to the squared Euclidean distance between the ith and jth observations. On the `USArrests` data, show that this proportionality holds.\n",
    "\n",
    "Hint: The Euclidean distance can be calculated using the `pairwise_distances()` function from the `sklearn.metrics` module, and correlations can be calculated using the `np.corrcoef()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc270fbd",
   "metadata": {},
   "source": [
    "### Percent Variance Explained\n",
    "\n",
    "A formula for calculating PVE was given as\n",
    "$$\\frac{\\sum_i^n z_{im}^2}{\\sum_j^p\\sum_i^n x_{ij}^2}=\\frac{\\sum_i^n (\\sum_j^p\\phi_{jm}x_{ij}    )^2}{\\sum_j^p\\sum_i^n x_{ij}^2}$$\n",
    "We also saw that the PVE can be obtained using the `explained_variance_ratio_` attribute of a fitted `PCA()` estimator.\n",
    "On the `USArrests` data, calculate PVE in two ways:\n",
    "\n",
    "(a) Using the `explained_variance_ratio_` output of the fitted `PCA()` estimator, as was done in Section 12.2.3.\n",
    "(b) By applying Equation 12.10 directly. The loadings are stored as the `components_` attribute of the fitted `PCA()` estimator. Use those loadings in Equation 12.10 to obtain the PVE.\n",
    "\n",
    "Hint: You will only obtain the same results in (a) and (b) if the same\n",
    "data is used in both cases. For instance, if in (a) you performed PCA()\n",
    "using centered and scaled variables, then you must center and scale\n",
    "the variables before applying Equation 12.10 in (b)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7adb03",
   "metadata": {},
   "source": [
    "### Hierarchical clustering of USA Arrest\n",
    "\n",
    "Consider the `USArrests` data. We will now perform hierarchical clustering on the states.\n",
    "(a) Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff6458",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45baf799",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "(b) Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670c8eef",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6849ba",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "(c) Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one.\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad919612",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acb5a56",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "(d) What effect does scaling the variables have on the hierarchical clustering obtained? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed? Provide a justification for your answer.\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2505c80a",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f5445b",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8dd8ce",
   "metadata": {},
   "source": [
    "### Simulation PCA and Kmeans\n",
    "\n",
    "In this problem, you will generate simulated data, and then perform PCA and K-means clustering on the data.\n",
    "\n",
    "(a) Generate a simulated data set with 20 observations in each of\n",
    "three classes (i.e. 60 observations total), and 50 variables.\n",
    "Hint: There are a number of functions in Python that you can\n",
    "use to generate data. One example is the normal() method of\n",
    "the random() function in numpy; the uniform() method is another\n",
    "option. Be sure to add a mean shift to the observations in each\n",
    "class so that there are three distinct classes.\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca4ceb1",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9e1156",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "(b) Perform PCA on the 60 observations and plot the first two principal\n",
    "component score vectors. Use a different color to indicate\n",
    "the observations in each of the three classes. If the three classes\n",
    "appear separated in this plot, then continue on to part (c). If\n",
    "not, then return to part (a) and modify the simulation so that\n",
    "there is greater separation between the three classes. Do not\n",
    "continue to part (c) until the three classes show at least some\n",
    "separation in the first two principal component score vectors.\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92e86f8",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81033cda",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "(c) Perform K-means clustering of the observations with K = 3.\n",
    "How well do the clusters that you obtained in K-means clustering\n",
    "compare to the true class labels?\n",
    "Hint: You can use the pd.crosstab() function in Python to compare\n",
    "the true class labels to the class labels obtained by clustering.\n",
    "Be careful how you interpret the results: K-means clustering\n",
    "will arbitrarily number the clusters, so you cannot simply check\n",
    "whether the true class labels and clustering labels are the same.\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef42580",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61800813",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "\n",
    "(d) Perform K-means clustering with K = 2. Describe your results.\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63509f41",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6f9940",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "(e) Now perform K-means clustering with K = 4, and describe your results.\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e615f1a8",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bf1040",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "(f) Now perform K-means clustering with K = 3 on the first two\n",
    "principal component score vectors, rather than on the raw data.\n",
    "That is, perform K-means clustering on the 60 × 2 matrix of\n",
    "which the first column is the first principal component score\n",
    "vector, and the second column is the second principal component\n",
    "score vector. Comment on the results.\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d422872b",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71ac44d",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "(g) Using the StandardScaler() estimator, perform K-means clustering\n",
    "with K = 3 on the data after scaling each variable to have\n",
    "standard deviation one. How do these results compare to those\n",
    "obtained in (b)? Explain.\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cb9bfa",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02b054f",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef271b1",
   "metadata": {},
   "source": [
    "### Matrix completion\n",
    "\n",
    "Write a Python function to perform matrix completion as in Algorithm\n",
    "12.1, and as outlined in Section 12.5.2. In each iteration, the\n",
    "function should keep track of the relative error, as well as the iteration\n",
    "count. Iterations should continue until the relative error is small\n",
    "enough or until some maximum number of iterations is reached (set a\n",
    "default value for this maximum number). Furthermore, there should\n",
    "be an option to print out the progress in each iteration.\n",
    "Test your function on the Boston data. First, standardize the features\n",
    "to have mean zero and standard deviation one using the\n",
    "`StandardScaler()` function. Run an experiment where you randomly\n",
    "leave out an increasing (and nested) number of observations from 5%\n",
    "to 30%, in steps of 5%. Apply Algorithm 12.1 with $M = 1, 2,\\dots, 8$.\n",
    "Display the approximation error as a function of the fraction of observations\n",
    "that are missing, and the value of M, averaged over 10\n",
    "repetitions of the experiment.\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796655e5",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41045167",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734e3044",
   "metadata": {},
   "source": [
    "### Matrix completion\n",
    "\n",
    "\n",
    "In Section 12.5.2, Algorithm 12.1 was implemented using the\n",
    "svd() function from the np.linalg module. However, given the connection\n",
    "between the svd() function and the PCA() estimator highlighted\n",
    "in the lab, we could have instead implemented the algorithm\n",
    "using PCA().\n",
    "Write a function to implement Algorithm 12.1 that makes use of PCA()\n",
    "rather than svd().\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fe64d3",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657afa9b",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83652e8",
   "metadata": {},
   "source": [
    "### Gene expression\n",
    "\n",
    "The dataset on gene expression (Ch12Ex13.csv) consists of 40 tissue samples with measurements on 1,000 genes. The first 20 samples are from healthy patients, while the second 20 are from a diseased group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fda108",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "credit_data=pd.read_csv(\"Ch12Ex13.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43713a57",
   "metadata": {},
   "source": [
    "(a) Load in the data using pd.read_csv(). You will need to select `header = None`.\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9746d29",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e2acd0",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "\n",
    "(b) Apply hierarchical clustering to the samples using correlationbased\n",
    "distance, and plot the dendrogram. Do the genes separate\n",
    "the samples into the two groups? Do your results depend on the\n",
    "type of linkage used?\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f43c1",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff44f13b",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "\n",
    "(c) Your collaborator wants to know which genes differ the most\n",
    "across the two groups. Suggest a way to answer this question,\n",
    "and apply it here.\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc851c",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfea996b",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee13d0b",
   "metadata": {},
   "source": [
    "## Additional Material"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709ca42e",
   "metadata": {},
   "source": [
    "### pheatmap\n",
    "\n",
    "pheatmap in R is popular in bioinformatics because it combines hierarchical clustering, heatmap, and k-means at once.  You can row cluster and column cluster at the same time. In python the closest you can find is in seaborn. https://seaborn.pydata.org/generated/seaborn.clustermap.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee678cf",
   "metadata": {
    "Rmd_chunk_options": "fig.width=7,fig.height=7.5",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris_pd = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= [\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"] + ['Species'])\n",
    "iris_pd[\"sample_id\"]=    list(range(1,151)  )\n",
    "iris_pd = iris_pd.set_index('sample_id')\n",
    "\n",
    "my_palette = dict(zip(iris_pd[\"Species\"].unique(), [\"orange\",\"yellow\",\"brown\"]))\n",
    "row_colors = iris_pd[\"Species\"].map(my_palette)\n",
    "tmp=sns.clustermap(iris_pd,  method=\"ward\", cmap=\"Blues\", standard_scale=1, row_colors=row_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f82fa27",
   "metadata": {},
   "source": [
    "### visualizing K-means result\n",
    "\n",
    "There is nice visualization for k-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a7cbaa",
   "metadata": {
    "Rmd_chunk_options": "fig.width=7,fig.height=7.5",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "feature, target = make_blobs(n_samples=600,\n",
    "                             centers=3,\n",
    "                             random_state=123,\n",
    "                             shuffle=True)\n",
    "\n",
    "kmeansfit = KMeans(n_clusters=3, random_state=0, n_init=\"auto\").fit(feature)\n",
    "\n",
    "df = pd.DataFrame({'x1': feature[:, 0], 'x2': feature[:, 1], 'target': target})\n",
    "plt.scatter(df[\"x1\"], df[\"x2\"], alpha = 0.6, s=10)\n",
    "plt.show()\n",
    "df['cluster'] = kmeansfit.fit_predict(feature)# get centroids\n",
    "centroids = kmeansfit.cluster_centers_\n",
    "cen_x = [i[0] for i in centroids]\n",
    "cen_y = [i[1] for i in centroids]\n",
    "## add to df\n",
    "df['cen_x'] = df.cluster.map({0:cen_x[0], 1:cen_x[1], 2:cen_x[2]})\n",
    "df['cen_y'] = df.cluster.map({0:cen_y[0], 1:cen_y[1], 2:cen_y[2]})# define and map colors\n",
    "colors = ['#DF2020', '#81DF20', '#2095DF']\n",
    "df['c'] = df.cluster.map({0:colors[0], 1:colors[1], 2:colors[2]})\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(8,8))\n",
    "# plot data\n",
    "plt.scatter(df[\"x1\"], df[\"x2\"], c=df.c, alpha = 0.3, s=10)\n",
    "\n",
    "# plot centroids\n",
    "plt.scatter(cen_x, cen_y, marker='*', c=colors, s=70)# plot Attack mean\n",
    "plt.show()\n",
    "#\n",
    "# plt.plot([df.x1.mean()]*2, [-2,2], color='black', lw=0.5, linestyle='--')\n",
    "# plt.xlim(-2,2)# plot Defense mean\n",
    "# plt.plot([-2,2], [df.x2.mean()]*2, color='black', lw=0.5, linestyle='--')\n",
    "# plt.ylim(-2,2)# create a list of legend elemntes\n",
    "# ## average line\n",
    "# legend_elements = [Line2D([0], [0], color='black', lw=0.5, linestyle='--', label='Average')]\n",
    "# ## markers / records\n",
    "# cluster_leg = [Line2D([0], [0], marker='o', color='w', label='Cluster {}'.format(i+1),\n",
    "#                markerfacecolor=mcolor, markersize=5) for i, mcolor in enumerate(colors)]\n",
    "# ## centroids\n",
    "# cent_leg = [Line2D([0], [0], marker='^', color='w', label='Centroid - C{}'.format(i+1),\n",
    "#             markerfacecolor=mcolor, markersize=10) for i, mcolor in enumerate(colors)]\n",
    "# # add all elements to the same list\n",
    "# legend_elements.extend(cluster_leg)\n",
    "# legend_elements.extend(cent_leg)\n",
    "# # plot legend\n",
    "# plt.legend(handles=legend_elements, loc='upper right', ncol=2)# title and labels\n",
    "# #plt.title('Pokemon Stats\\n', loc='left', fontsize=22)\n",
    "# plt.xlabel('x1')\n",
    "# plt.ylabel('x2')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a318c6",
   "metadata": {},
   "source": [
    "Another nice way to visualize K-means results when there are not too many k is the radar chart.\n",
    "Returning to the USArrest example, we can use k-means and then inspect whether the clusters make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abfff2a",
   "metadata": {
    "Rmd_chunk_options": "fig.width=7,fig.height=7.5",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "\n",
    "USArrests = pd.read_csv('USArrests.csv')#\n",
    "USArrests=USArrests.rename(columns={\"Unnamed: 0\":\"State\"})\n",
    "USArrests_kmeansfit = KMeans(n_clusters=5, random_state=0, n_init=\"auto\").fit(std_scaler.fit_transform(USArrests.loc[:,[\"Murder\",\"Assault\",  \"UrbanPop\",  \"Rape\"]]))\n",
    "\n",
    "df = pd.DataFrame(USArrests_kmeansfit.cluster_centers_,columns=[\"Murder\",\"Assault\",  \"UrbanPop\",  \"Rape\"])\n",
    "df[\"Cluster\"]=list(range(1,6))\n",
    "USArrests[\"Cluster\"]=USArrests_kmeansfit.fit_predict(USArrests.loc[:,[\"Murder\",\"Assault\",  \"UrbanPop\",  \"Rape\"]])\n",
    "# USA_cluster_mean=USArrests.loc[:,[\"Murder\",\"Assault\",  \"UrbanPop\",  \"Rape\",\"Cluster\"]].groupby(\"Cluster\").mean(numeric_only=True)\n",
    "# USA_cluster_mean[\"Cluster\"]=USA_cluster_mean.index\n",
    "USA_x_m=pd.melt(df, id_vars=['Cluster'],\n",
    "          value_vars=[\"Murder\",\"Assault\",  \"UrbanPop\",  \"Rape\"])\n",
    "fig = px.line_polar(USA_x_m, r = 'value', theta = 'variable', line_close = True,\n",
    "                    color = 'Cluster')\n",
    "#fig.update_traces(fill = 'toself')\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "states = gpd.read_file('cb_2018_us_state_500k')\n",
    "\n",
    "non_continental = ['HI','VI','MP','GU','AK','AS','PR']\n",
    "us49 = states\n",
    "for n in non_continental:\n",
    "    us49 = us49[us49.STUSPS != n]\n",
    "states=us49\n",
    "# Assuming km.usA$cluster is your clustering result\n",
    "# Replace the following line with your clustering result\n",
    "km_usA_cluster = {'cluster': [0, 1, 2, 3], 'region': ['alabama', 'alaska', 'arizona', 'arkansas']}  # Sample data\n",
    "\n",
    "# Convert km.usA$cluster to a DataFrame\n",
    "dfft = pd.DataFrame(USArrests)\n",
    "\n",
    "# Convert region names to lowercase to match with states data\n",
    "dfft['region'] = dfft['State'].str.lower()\n",
    "\n",
    "colors=sns.color_palette(\"husl\",5).as_hex()\n",
    "\n",
    "# Merge the states GeoDataFrame with the clustering results DataFrame\n",
    "statec = states.merge(dfft, left_on='NAME', right_on='State', how='right')\n",
    "statec['c'] = statec.Cluster.map({0:colors[0], 1:colors[1], 2:colors[2],3:colors[3],4:colors[4]})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "fig.subplots_adjust(hspace=0.0, wspace=0.0)\n",
    "statec.plot(\"Cluster\", color=statec.c,legend=True,ax=ax)\n",
    "xmin, ymin, xmax, ymax = states.total_bounds\n",
    "pad = 5  # add a padding around the geometry\n",
    "ax.set_xlim(xmin-pad, xmax+pad)\n",
    "ax.set_ylim(ymin-pad, ymax+pad)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66e0a46",
   "metadata": {},
   "source": [
    "### Choice of the distance and linkage\n",
    "\n",
    "Weird things happen for a particular choice of linkage methods.\n",
    "\n",
    "This is an example where using the centroid flips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c6988",
   "metadata": {
    "Rmd_chunk_options": "fig.width=8,fig.height=3",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "x=[5.24,4.58,6.25,6.15,5.02,5.95]\n",
    "y=[3.16,2.78,2.24,4.04,3.99,2.77]\n",
    "data = list(zip(x, y))\n",
    "\n",
    "print(data)\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(1, 3)\n",
    "ax0.scatter(x,y)\n",
    "linkage_data = linkage(data, method='single', metric='euclidean')\n",
    "dendrogram(linkage_data,ax=ax1)\n",
    "linkage_data = linkage(data, method='centroid', metric='euclidean')\n",
    "dendrogram(linkage_data,ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6ccb21",
   "metadata": {},
   "source": [
    "The different combinations will return different results.  Knowing that you cannot be ignorant about the method is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7f0477",
   "metadata": {},
   "source": [
    "### Other distances\n",
    "\n",
    "There are many different distances and some are good for binary variables.\n",
    "You can find most of them in `scipy.spatial.distance`\n",
    "https://docs.scipy.org/doc/scipy/reference/spatial.distance.html#module-scipy.spatial.distance\n",
    "\n",
    "- euclidean\t$$d_{jk} = \\sqrt{(\\sum(x_{ij}-x_{ik})^2)}$$\n",
    "\tbinary:$sqrt(A+B-2*J)$\n",
    "- manhattan\t$$d_{jk} = \\sum(|x_{ij} - x_{ik}|)$$\n",
    "\tbinary:$A+B-2*J$\n",
    "- gower\t$$d_{jk}  = \\frac{1}{M} \\sum\\left(\\frac{|x_{ij}-x_{ik}|}{(\\max(x_{i})-\\min(x_{i}))}\\right)$$\n",
    "\tbinary:$(A+B-2*J)/M$,\n",
    "\twhere M is the number of columns (excluding missing values)\n",
    "- altGower\t$$d_{jk}  = ( \\frac{1}{NZ} ) \\sum(|x_{ij} - x_{ik}|)$$\n",
    "\twhere $NZ$ is the number of non-zero columns excluding double-zeros (Anderson et al. 2006).\n",
    "\tbinary:$(A+B-2*J)/(A+B-J)$\n",
    "- canberra\t$$d_{jk} = \\frac{1}{NZ}  \\sum \\left(\\frac{(x_{ij}-x_{ik})}{(x_{ij}+x_{ik})}\\right)$$\n",
    "\twhereNZis the number of non-zero entries.\n",
    "\tbinary:$(A+B-2*J)/(A+B-J)$\n",
    "- bray\t$$d_{jk}  = \\left(\\sum |x_{ij}-x_{ik}| \\right)/\\left(\\sum (x_{ij}+x_{ik})\\right)$$\n",
    "\tbinary:$(A+B-2*J)/(A+B)$\n",
    "- kulczynski\t$$d_{jk} = 1 - 0.5*((\\sum \\min(x_{ij},x_{ik})/(\\sum x_{ij}) + (\\sum min(x_{ij},x_{ik})/(\\sum x_{ik}))$$\n",
    "\tbinary:$1-(J/A + J/B)/2$\n",
    "- morisita\t$$d_{jk} = 1 - 2*\\frac{\\sum\\left(x_{ij}*x_{ik}\\right)}{((\\lambda_{j}+\\lambda_{k}) * \\sum(x_{ij})*\\sum(x_{ik}))}$$, where\n",
    "\t$\\lambda_{j} = \\sum(x_{ij}*(x_{ij}-1))/\\sum(x_{ij})*\\sum(x_{ij}-1)$\n",
    "\n",
    "\tbinary: cannot be calculated\n",
    "\n",
    "- horn\t$$d_{jk} = 1 - 2*\\frac{\\sum\\left(x_{ij}*x_{ik}\\right)}{((\\lambda_{j}+\\lambda_{k}) * \\sum(x_{ij})*\\sum(x_{ik}))}$$ ( Same as morisita ),\n",
    " but $$\\lambda_{j} = \\sum(x_{ij}^2)/(\\sum(x[ij])^2)$$\n",
    "\tbinary:$(A+B-2*J)/(A+B)$\n",
    "- binomial\t$$d_{jk} = \\sum\\left(x_{ij}*\\log(\\frac{x_{ij}}{n_{i}}) + x_{ik}*\\log(\\frac{x_{ik}}{n_{i}}) - n_{i}*\\log( \\frac{1}{2} )\\right)/n_{i}$$,\n",
    "\twhere $n_{i} = x_{ij} + x_{ik}$\n",
    "\n",
    "\tbinary:$\\log(2)*(A+B-2*J)$\n",
    "\n",
    "- cao\t$$d_{jk}  = \\frac{1}{S} * sum(log(n_{i}/2) - (x_{ij}*\\log(x_{ik}) + x_{ik}*\\log(x_{ij}))/n_{i})$$,\n",
    "\twhere $S$ is the number of species in compared sites and $n_{i} = x_{ij} + x_{ik}$\n",
    "\n",
    "You can find these in the vegan package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1137b4b",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "pdist_Methods=['braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation', 'cosine', 'dice', 'euclidean', 'hamming', 'jaccard', 'jensenshannon', 'kulczynski1', 'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']\n",
    "\n",
    "xx = np.transpose(np.random.binomial(n=1, p=0.5, size=(1000, 100)))\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "pairwise_distances(xx,metric=\"jaccard\")\n",
    "\n",
    "from scipy.spatial.distance import pdist\n",
    "pdist(xx, 'euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd1051e",
   "metadata": {},
   "source": [
    "### Cluster performance indices\n",
    "\n",
    "There are various indices proposed for comparing the clustering result.\n",
    "Few of the popular ones are implemented in `sklearn.metrics`\n",
    "|----------------------------------|------------------------------------------------------\n",
    "| ‘adjusted_mutual_info_score’     | metrics.adjusted_mutual_info_score\n",
    "| ‘adjusted_rand_score’            | metrics.adjusted_rand_score\n",
    "| ‘completeness_score’             | metrics.completeness_score\n",
    "| ‘fowlkes_mallows_score’          | metrics.fowlkes_mallows_score\n",
    "| ‘homogeneity_score’              | metrics.homogeneity_score\n",
    "| ‘mutual_info_score’              | metrics.mutual_info_score\n",
    "| ‘normalized_mutual_info_score’   | metrics.normalized_mutual_info_score\n",
    "| ‘rand_score’                     | metrics.rand_score\n",
    "| ‘v_measure_score’                | metrics.v_measure_score\n",
    "\n",
    "Here is synthetic data with 6 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6845a93c",
   "metadata": {
    "Rmd_chunk_options": "fig.width=7,fig.height=7.5",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "feature, target = make_blobs(n_samples=600,\n",
    "                             centers=6,\n",
    "                             random_state=123,\n",
    "                             shuffle=True)\n",
    "mydata = pd.concat([pd.DataFrame(feature, columns=['x1', 'x2']), pd.DataFrame(target, columns=['y'])], axis=1)\n",
    "plt.scatter(mydata[\"x1\"], mydata[\"x2\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe0fa95",
   "metadata": {},
   "source": [
    "If we manually calculate the WGSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593c68d1",
   "metadata": {
    "Rmd_chunk_options": "eval=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Assuming `mydata` is a numpy array or pandas DataFrame containing your data\n",
    "wss=np.zeros(15)\n",
    "wss[0] = (mydata.shape[0] - 1) * np.sum(np.apply_along_axis(np.var, 0, mydata))\n",
    "\n",
    "for i in range(2, 16):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=0, n_init=\"auto\")\n",
    "    kmeans.fit(mydata)\n",
    "    wss[i - 1] = kmeans.inertia_\n",
    "\n",
    "plt.plot(range(1, 16), wss, marker='o', linestyle='-')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Within groups sum of squares')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6a3bc7",
   "metadata": {},
   "source": [
    "The WGSS stops decreasing around K=5 to 6, which is correct for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d3f23",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=6, random_state=0, n_init=\"auto\")\n",
    "kmeans.fit(mydata)\n",
    "#mydata[\"cluster\"]= kmeans.labels_\n",
    "plt.scatter(mydata[\"x1\"], mydata[\"x2\"],c=kmeans.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1422d363",
   "metadata": {},
   "source": [
    "#### [silhouette score](https://en.wikipedia.org/wiki/Silhouette_(clustering))\n",
    "\n",
    "Silhouette Scores are also popular metric to look at in clustering problem. Score near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster.\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b18e95",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "silhouette_avg = silhouette_score(mydata, kmeans.labels_)\n",
    "sample_silhouette_values = silhouette_samples(mydata, kmeans.labels_)\n",
    "\n",
    "ses=np.zeros(14)\n",
    "for i in range(2, 16):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=0, n_init=\"auto\")\n",
    "    kmeans.fit(mydata)\n",
    "    ses[i - 2] = silhouette_score(mydata, kmeans.labels_)\n",
    "\n",
    "\n",
    "plt.plot(range(1, 15), ses, marker='o', linestyle='-')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.title('Silhouette Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7fbd2",
   "metadata": {
    "Rmd_chunk_options": "echo=FALSE",
    "jupyter": {
     "output_hidden": false,
     "source_hidden": true
    },
    "kernel": "Python3",
    "tags": [
     "report_output"
    ]
   },
   "outputs": [],
   "source": [
    "range_n_clusters = [2, 3, 4, 5, 6, 7]\n",
    "\n",
    "n_clusters=6\n",
    "# Create a subplot with 1 row and 2 columns\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_size_inches(18, 7)\n",
    "\n",
    "# The 1st subplot is the silhouette plot\n",
    "# The silhouette coefficient can range from -1, 1 but in this example all\n",
    "# lie within [-0.1, 1]\n",
    "ax1.set_xlim([-0.1, 1])\n",
    "# The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "# plots of individual clusters, to demarcate them clearly.\n",
    "ax1.set_ylim([0, len(mydata) + (n_clusters + 1) * 10])\n",
    "\n",
    "# Initialize the clusterer with n_clusters value and a random generator\n",
    "# seed of 10 for reproducibility.\n",
    "clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "cluster_labels = clusterer.fit_predict(mydata)\n",
    "\n",
    "# The silhouette_score gives the average value for all the samples.\n",
    "# This gives a perspective into the density and separation of the formed\n",
    "# clusters\n",
    "silhouette_avg = silhouette_score(mydata, cluster_labels)\n",
    "print(\n",
    "    \"For n_clusters =\",\n",
    "    n_clusters,\n",
    "    \"The average silhouette_score is :\",\n",
    "    silhouette_avg,\n",
    ")\n",
    "\n",
    "# Compute the silhouette scores for each sample\n",
    "sample_silhouette_values = silhouette_samples(mydata, cluster_labels)\n",
    "\n",
    "y_lower = 10\n",
    "\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    # Aggregate the silhouette scores for samples belonging to\n",
    "    # cluster i, and sort them\n",
    "    ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "    import matplotlib.cm as cm\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "    ax1.fill_betweenx(\n",
    "        np.arange(y_lower, y_upper),\n",
    "        0,\n",
    "        ith_cluster_silhouette_values,\n",
    "        facecolor=color,\n",
    "        edgecolor=color,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "    # Label the silhouette plots with their cluster numbers at the middle\n",
    "    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "    # Compute the new y_lower for next plot\n",
    "    y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "\n",
    "ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "# The vertical line for average silhouette score of all the values\n",
    "ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "# 2nd Plot showing the actual clusters formed\n",
    "colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "ax2.scatter(\n",
    "    mydata.loc[:, \"x1\"],mydata.loc[:, \"x2\"], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    ")\n",
    "\n",
    "# Labeling the clusters\n",
    "centers = clusterer.cluster_centers_\n",
    "# Draw white circles at cluster centers\n",
    "ax2.scatter(\n",
    "    centers[:, 0],\n",
    "    centers[:, 1],\n",
    "    marker=\"o\",\n",
    "    c=\"white\",\n",
    "    alpha=1,\n",
    "    s=200,\n",
    "    edgecolor=\"k\",\n",
    ")\n",
    "\n",
    "for i, c in enumerate(centers):\n",
    "    ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "ax2.set_title(\"The visualization of the clustered data.\")\n",
    "ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
    "    % n_clusters,\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eece40",
   "metadata": {},
   "source": [
    "### Partitioning Around Medoids\n",
    "K-Medoids or Partitioning (clustering) of the data into k clusters “Around Medoids” (PAM) is a more robust version of K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f4ee45",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#pip install scikit-learn-extra\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "kmedoids = KMedoids(n_clusters=3, random_state=0).fit(iris.data)\n",
    "kmedoids.labels_\n",
    "kmedoids.cluster_centers_\n",
    "kmedcrsstab = pd.crosstab(index=iris.target,\n",
    "                           columns=kmedoids.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f1e67f",
   "metadata": {},
   "source": [
    "### Mclust\n",
    "\n",
    "Mclust is an R package with a collection of model based clustering methods.  mclustpy is the python rapper of the R package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b3bb03",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# pip install rpy2\n",
    "# pip install mclustpy\n",
    "from mclustpy import mclustpy\n",
    "mc = mclustpy(iris.data, G=9, modelNames='EEE', random_seed=2020)\n",
    "mc['classification']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c5c5f0",
   "metadata": {},
   "source": [
    "### Unsupervised Learning for Timeseries\n",
    "\n",
    "Working with timeseries data in Unsupervised Learning situations poses several unique challenges.\n",
    "The naive way of treating observations along time as a set of features works well only when the timeseries' are well aligned.\n",
    "For many realistic situations, this is not necessarily the case. Even in a situation where the comparison is about the aligned timeseries, there are problems such as missing data and misalignment of the starting times that need treatment.\n",
    "https://github.com/aapatel09/handson-unsupervised-learning/blob/master/13_temporal_clustering.ipynb\n",
    "To make the point, here is CO2 emission by country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a3c07e",
   "metadata": {
    "Rmd_chunk_options": "echo=FALSE,eval=TRUE",
    "jupyter": {
     "output_hidden": false,
     "source_hidden": true
    },
    "kernel": "Python3",
    "tags": [
     "report_output"
    ]
   },
   "outputs": [],
   "source": [
    "# pip install openpyxl\n",
    "#https://github.com/opencasestudies/ocs-bp-co2-emissions/tree/master/data/raw\n",
    "CO2_emissions = pd.read_excel(\"yearly_co2_emissions_1000_tonnes.xlsx\")\n",
    "CO2_emissions_subset=CO2_emissions[CO2_emissions.country.isin([\"United States\",\"Canada\",\"China\",\"United Kingdom\",\"France\",\"India\"])]\n",
    "memm=pd.melt(CO2_emissions_subset, id_vars=['country'])\n",
    "#as.integer(as.character(variable))\n",
    "(\n",
    "  ggplot(memm)+geom_point()+geom_line()+\n",
    "  aes(x=\"variable\",y=\"value\",color=\"country\")+\n",
    "  scale_y_log10()+xlab(\"year\")+ylab(\"Co2 Emission\") +\n",
    "  theme(axis_text_x = element_text(angle = 45, hjust = 1, vjust = 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3dee1a",
   "metadata": {},
   "source": [
    "Let's say you want to cluster the countries by their trend.  The time is aligned since each country has the same years.\n",
    "However, you can see that the measurement goes back much longer in time for UK than other countries. Also there are gaps in the measurements that need to be taken care of.  One way to deal with such data is to limit your observations to years that are more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6ea24e",
   "metadata": {
    "Rmd_chunk_options": "eval=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "memm2000=memm[memm.variable>2000]\n",
    "(\n",
    "  ggplot(memm2000)+\n",
    "  geom_point()+geom_line()+\n",
    "  aes(x=\"variable\",y=\"value\",color=\"country\",group=\"country\")+\n",
    "  scale_y_log10()+xlab(\"year\")+ylab(\"Co2 Emission\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52cd6d3",
   "metadata": {},
   "source": [
    "Another example is to determine the letter being written from the trajectories of a writer. Below are lines for five examples of the X velocity for A and B.\n",
    "https://archive.ics.uci.edu/dataset/175/character+trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4db03d",
   "metadata": {
    "Rmd_chunk_options": "eval=T",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "CharTraj=pd.read_csv(\"CharTraj.csv\")\n",
    "plt.plot(range(0,203), CharTraj.iloc[0,2:205])\n",
    "plt.plot(range(0,203), CharTraj.iloc[1,2:205])\n",
    "plt.plot(range(0,203), CharTraj.iloc[2,2:205])\n",
    "plt.plot(range(0,203), CharTraj.iloc[3,2:205])\n",
    "plt.plot(range(0,203), CharTraj.iloc[4,2:205])\n",
    "plt.show()\n",
    "plt.plot(range(0,203), CharTraj.iloc[5,2:205])\n",
    "plt.plot(range(0,203), CharTraj.iloc[6,2:205])\n",
    "plt.plot(range(0,203), CharTraj.iloc[7,2:205])\n",
    "plt.plot(range(0,203), CharTraj.iloc[8,2:205])\n",
    "plt.plot(range(0,203), CharTraj.iloc[9,2:205])\n",
    "plt.show()\n",
    "# import scipy.io\n",
    "# CharTraj = scipy.io.loadmat('character_trajectories/mixoutALL_shifted.mat')\n",
    "# from matplotlib.lines import Line2D\n",
    "# plt.plot(range(0,178), CharTraj.get(\"mixout\")[0][1][1,:])\n",
    "# Line2D(range(0,178), CharTraj.get(\"mixout\")[0][2][1,:])\n",
    "# Line2D(range(0,178), CharTraj.get(\"mixout\")[0][3][1,:])\n",
    "# Line2D(range(0,178), CharTraj.get(\"mixout\")[0][4][1,:])\n",
    "# Line2D(range(0,178), CharTraj.get(\"mixout\")[0][6][1,:])\n",
    "# plt.show()\n",
    "#\n",
    "# plt.plot(range(0,130), CharTraj.get(\"mixout\")[0][174][1,:])\n",
    "# plt.plot(range(0,130), CharTraj.get(\"mixout\")[0][175][1,:])\n",
    "# plt.plot(range(0,130), CharTraj.get(\"mixout\")[0][176][1,:])\n",
    "# plt.plot(range(0,130), CharTraj.get(\"mixout\")[0][177][1,:])\n",
    "# plt.plot(range(0,130), CharTraj.get(\"mixout\")[0][178][1,:])\n",
    "# plt.show()\n",
    "#\n",
    "# (CharTraj.get(\"consts\")[0][0][3])\n",
    "# pd.DataFrame(CharTraj.get(\"consts\")[0][0][4][0]).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcec9c03",
   "metadata": {},
   "source": [
    "This will be a whole timeseries comparison within fixed time window, but you have a misalignment in time that would distort point-by-point distance.\n",
    "\n",
    "Suppose the goal is to extract particular activity from measurements taken on a wearable device and to classify what is being done. In that case, an additional segmentation issue must be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bca7b0",
   "metadata": {},
   "source": [
    "#### Dynamic Time Warping (DTW)\n",
    "\n",
    "Dynamic Time warping is a way to calculate distances between datapoints when the datapoints are shifted horizontally in time between each other, but the shape is consistent.  Unlike Euclidean distance, for DTW, the two time series do not have to have an equal length. DTW calculates the smallest distance between all points and then finds the closest matching points.  Here is an illustration of how it works. (https://rpubs.com/esobolewska/dtw-time-series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6734e1f2",
   "metadata": {
    "Rmd_chunk_options": "echo=FALSE,eval=TRUE",
    "jupyter": {
     "output_hidden": false,
     "source_hidden": true
    },
    "kernel": "Python3",
    "tags": [
     "report_output"
    ]
   },
   "outputs": [],
   "source": [
    "# pip install dtw-python # you will need to install\n",
    "from dtw import dtw as dtwpy_dtw\n",
    "from dtw import *\n",
    "#pip install dtaidistance # you will need to install\n",
    "from dtaidistance import dtw as dtad_dtw\n",
    "#pip install tslearn # you will need to install\n",
    "# from tslearn.metrics import dtw as ts_dtw\n",
    "\n",
    "## A noisy sine wave as query\n",
    "idx = np.linspace(0,6.28,num=100)\n",
    "query = np.sin(idx) + np.random.uniform(size=100)/10.0\n",
    "\n",
    "## A cosine is for template; sin and cos are offset by 25 samples\n",
    "template = np.cos(idx)\n",
    "\n",
    "a1 = query# np.array([7, 9, 6, 9, 12, 6, 4, 6, 8])\n",
    "a2 = template#np.array([5, 6, 4, 3, 9, 5, 6, 8, 9])\n",
    "\n",
    "# Calculate Dynamic Time Warping (DTW)\n",
    "dtad_distance = dtad_dtw.distance(a1, a2)\n",
    "#ts_distance   = ts_dtw(a1, a2) # problem with reticulate\n",
    "#\n",
    "ptwpy_distance = dtwpy_dtw(a1, a2, keep_internals=True)\n",
    "\n",
    "## Display the warping curve, i.e. the alignment curve\n",
    "ptwpy_distance.plot(type=\"threeway\")\n",
    "\n",
    "# Plotting\n",
    "xrange = [np.min(a1),np.max(a1)+1]\n",
    "yrange = [np.min(a2), np.max(a2)+ 1]\n",
    "\n",
    "plt.plot(xrange, yrange, color='white')  # Set up the plot\n",
    "plt.plot(a1, color='blue', linestyle='-')  # Plot a1\n",
    "plt.plot(a2, color='magenta', linestyle='-')  # Plot a2\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d803eae5",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "dtw(a1, a2, keep_internals=True,\n",
    "    step_pattern=rabinerJuangStepPattern(6, \"c\"))\\\n",
    "    .plot(type=\"twoway\",offset=-2)\n",
    "plt.show()\n",
    "## See the recursion relation, as formula and diagram\n",
    "print(rabinerJuangStepPattern(6,\"c\"))\n",
    "rabinerJuangStepPattern(6,\"c\").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008441a5",
   "metadata": {},
   "source": [
    "using pyts library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b5b88d",
   "metadata": {
    "Rmd_chunk_options": "eval=FALSE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#pip install pyts # you will need to install\n",
    "from pyts.metrics import dtw as pyts_dtw\n",
    "\n",
    "pyts_distance = pyts_dtw(a1, a2) # problem with reticulate\n",
    "n_timestamps_1, n_timestamps_2 = a1.size,a2.size\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "timestamps_1 = np.arange(n_timestamps_1 + 1)\n",
    "timestamps_2 = np.arange(n_timestamps_2 + 1)\n",
    "\n",
    "# Dynamic Time Warping: classic\n",
    "dtw_classic, path_classic = pyts_dtw(a1, a2, dist='square',\n",
    "                                method='classic', return_path=True)\n",
    "matrix_classic = np.zeros((n_timestamps_1, n_timestamps_2))\n",
    "matrix_classic[tuple(path_classic)] = 1.\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.pcolor(timestamps_1, timestamps_2, matrix_classic.T,\n",
    "           edgecolors='k', cmap='Greys')\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title(\"{0}\\nDTW(x, y) = {1:.2f}\".format('classic', dtw_classic),\n",
    "          fontsize=14)\n",
    "plt.show()\n",
    "# plot(dtwfit, xlab=\"a1 - blue\", ylab=\"a2 - magenta\",\n",
    "#      xaxp  = c(0,10,10), yaxp = c(0,10,10), type=\"threeway\")\n",
    "# plot(dtwfit, xaxp  = c(0,10,10), yaxp = c(0,10,10), type=\"twoway\", col=c('blue', 'magenta'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf459f41",
   "metadata": {},
   "source": [
    "tslearn.clustering has few useful functions.\n",
    "But for some reason it does not work with Rstudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ebc6a1",
   "metadata": {
    "Rmd_chunk_options": "eval=FALSE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.clustering import KShape\n",
    "scaler = StandardScaler()\n",
    "co2emissions_norm=pd.DataFrame(scaler.fit_transform(CO2_emissions.iloc[:,254:265].dropna()),columns=CO2_emissions.iloc[:,254:265].columns)\n",
    "\n",
    "N_CLUSTERS=5\n",
    "model = TimeSeriesKMeans(n_clusters=N_CLUSTERS, metric=\"dtw\",\n",
    "                         max_iter=10, random_state=1234)\n",
    "model.fit(co2emissions_norm)\n",
    "y_pred = model.predict(co2emissions_norm)\n",
    "plt.figure()\n",
    "#plt.subplots(figsize = (20, 16))\n",
    "for yi in range(N_CLUSTERS):\n",
    "    plt.subplot(10, 6, yi + 1)\n",
    "    tempx=co2emissions_norm[y_pred == yi]\n",
    "    for xx in range(0,tempx.shape[0]):\n",
    "        plt.plot(tempx.iloc[xx,:].ravel(), \"k-\", alpha=.2)\n",
    "    plt.plot(model.cluster_centers_[yi].ravel(), \"r-\")\n",
    "    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),\n",
    "             transform=plt.gca().transAxes)\n",
    "    if yi == 1:\n",
    "        plt.title(\"Euclidean $k$-means\")\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f88db3",
   "metadata": {},
   "source": [
    "For the hand writing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e08e6",
   "metadata": {
    "Rmd_chunk_options": "eval=F",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "CharTraj_fill=CharTraj.iloc[:,2:205].fillna(0)\n",
    "kshape=KShape(n_clusters=20,\n",
    "                         max_iter=10, random_state=1234)\n",
    "kshape.fit(CharTraj_fill)\n",
    "# hc <- tsclust(CharTraj, type = \"hierarchical\", k = 20L,\n",
    "#               distance = \"sbd\", trace = TRUE,\n",
    "#               control = hierarchical_control(method = \"average\"))\n",
    "# plot(hc)\n",
    "\n",
    "\n",
    "# tdf<-melt(CharTraj) # %>% group_by(L1) %>% mutate(id = row_number())\n",
    "# tcf<-melt(hc@cluster)\n",
    "# tcf$L1<-rownames(tcf)\n",
    "# tdf<-merge(tdf,tcf,by =\"L1\")\n",
    "# tdf$num <- sequence(rle(tdf$L1)$lengths)\n",
    "# ggplot(tdf)+geom_line(aes(x=num,y=value.x,group=L1))+facet_wrap(~value.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883c02fc",
   "metadata": {},
   "source": [
    "### DBScan\n",
    "\n",
    "The DBScan clustering algorithm is as follows:\n",
    "\n",
    "1. Randomly select a point i.\n",
    "2. Retrieve all the points that are density reachable from i concerning the Maximum radius of the neighborhood (EPS) and the minimum number of points within eps neighborhood(Min Pts).\n",
    "3. If the number of points in the neighborhood is more than Min Pts then i is a core point.\n",
    "4. For i core points, a cluster is formed. If i is not a core point, mark it as a noise/outlier and move to the next point.\n",
    "5. Continue the process until all the points have been processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeedfb1",
   "metadata": {
    "Rmd_chunk_options": "fig.width=7,fig.height=7.5",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "clustering = DBSCAN(eps=0.45, min_samples=5).fit(iris.data)\n",
    "\n",
    "# Checking cluster\n",
    "labels=clustering.labels_\n",
    "\n",
    "# Table\n",
    "crsstab = pd.crosstab(index=iris.target,\n",
    "                           columns=clustering.labels_)\n",
    "\n",
    "# Plotting Cluster\n",
    "\n",
    "unique_labels = set(labels)\n",
    "core_samples_mask = np.zeros_like(labels, dtype=bool)\n",
    "core_samples_mask[clustering.core_sample_indices_] = True\n",
    "\n",
    "plt.scatter(iris.data[:, 2], iris.data[:, 3])\n",
    "plt.show()\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = labels == k\n",
    "\n",
    "    xy = iris.data[class_member_mask & core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 2],\n",
    "        xy[:, 3],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=14,\n",
    "    )\n",
    "\n",
    "    xy = iris.data[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 2],\n",
    "        xy[:, 3],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=6,\n",
    "    )\n",
    "\n",
    "#plt.title(f\"Estimated number of clusters: {n_clusters_}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e484361b",
   "metadata": {},
   "source": [
    "### Other dimension reduction techniques\n",
    "\n",
    "Various other dimension reduction techniques have been popularized over the years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f06c88",
   "metadata": {},
   "source": [
    "####  t-distributed stochastic neighbor embedding (t-SNE)\n",
    "\n",
    "First is [t-SNE](https://lvdmaaten.github.io/tsne/) by L.J.P. van der Maaten and G.E. Hinton (2008). The algorithm projects the high-dimensional data points into low dimension (2D) by inducing the projected data to have a similar distribution as the original data points by minimizing KL divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ad50e7",
   "metadata": {
    "Rmd_chunk_options": "fig.width=7,fig.height=7.5,eval=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "                   init='random', perplexity=3).fit_transform(fm.iloc[1:200,1:785])\n",
    "X_embedded.shape\n",
    "\n",
    "\n",
    "fig = px.scatter(x=X_embedded[:, 0], y=X_embedded[:, 1], color=fm.iloc[1:200,0])\n",
    "fig.update_layout(\n",
    "    title=\"t-SNE visualization of Custom Classification dataset\",\n",
    "    xaxis_title=\"First t-SNE\",\n",
    "    yaxis_title=\"Second t-SNE\",\n",
    ")\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4638ac4a",
   "metadata": {},
   "source": [
    "#### Uniform Manifold Approximation (UMAP)\n",
    "\n",
    "UMAP is another dimension reduction described by McInnes and Healy (2018) in <arXiv:1802.03426>.\n",
    "\n",
    "https://umap-learn.readthedocs.io/en/latest/basic_usage.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3e94bc",
   "metadata": {
    "Rmd_chunk_options": "eval=FALSE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#pip install tensorflow\n",
    "#pip install umap-learn\n",
    "#import tensorflow\n",
    "#fm=tensorflow.keras.datasets.fashion_mnist.load_data()\n",
    "import umap\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "fig, ax_array = plt.subplots(20, 20)\n",
    "axes = ax_array.flatten()\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(digits.images[i], cmap='gray_r')\n",
    "\n",
    "\n",
    "plt.setp(axes, xticks=[], yticks=[], frame_on=False)\n",
    "plt.tight_layout(h_pad=0.5, w_pad=0.01)\n",
    "plt.show()\n",
    "\n",
    "digits_df = pd.DataFrame(digits.data[:,1:11])\n",
    "digits_df['digit'] = pd.Series(digits.target).map(lambda x: 'Digit {}'.format(x))\n",
    "sns.pairplot(digits_df, hue='digit', palette='Spectral');\n",
    "\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "reducer.fit(digits.data)\n",
    "embedding = reducer.transform(digits.data)\n",
    "# Verify that the result of calling transform is\n",
    "# idenitical to accessing the embedding_ attribute\n",
    "assert(np.all(embedding == reducer.embedding_))\n",
    "embedding.shape\n",
    "\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], c=digits.target, cmap='Spectral', s=5)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "plt.title('UMAP projection of the Digits dataset', fontsize=24);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac9069c",
   "metadata": {},
   "source": [
    "### Using H2O for Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcf1387",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import h2o\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00225169",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Import the birds dataset into H2O:\n",
    "birds = h2o.import_file(\"https://s3.amazonaws.com/h2o-public-test-data/smalldata/pca_test/birds.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bbc354",
   "metadata": {},
   "source": [
    "#### PCA\n",
    "\n",
    "H2O uses the power method to calculate the singular value decomposition of the Gram matrix.\n",
    "https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/pca.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d38784",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from h2o.estimators import H2OPrincipalComponentAnalysisEstimator\n",
    "\n",
    "# Split the dataset into a train and valid set:\n",
    "train, valid = birds.split_frame(ratios = [.8], seed = 1234)\n",
    "\n",
    "# Build and train the model:\n",
    "birds_pca = H2OPrincipalComponentAnalysisEstimator(k = 5,\n",
    "                                                   use_all_factor_levels = True,\n",
    "                                                   pca_method = \"glrm\",\n",
    "                                                   transform = \"standardize\",\n",
    "                                                   impute_missing = True)\n",
    "birds_pca.train(training_frame = train)\n",
    "\n",
    "# Generate predictions on a validation set (if necessary):\n",
    "pred = birds_pca.predict(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f965b27b",
   "metadata": {},
   "source": [
    "#### K-means\n",
    "\n",
    "https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/k-means.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd69bf2",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from h2o.estimators import H2OKMeansEstimator\n",
    "# Build and train the model:\n",
    "\n",
    "# Import the iris dataset into H2O:\n",
    "iris = h2o.import_file(\"http://h2o-public-test-data.s3.amazonaws.com/smalldata/iris/iris_wheader.csv\")\n",
    "\n",
    "# Set the predictors:\n",
    "predictors = [\"sepal_len\", \"sepal_wid\", \"petal_len\", \"petal_wid\"]\n",
    "# Split the dataset into a train and valid set:\n",
    "train, valid = iris.split_frame(ratios=[.8], seed=1234)\n",
    "\n",
    "\n",
    "iris_kmeans = H2OKMeansEstimator(k=10,\n",
    "                                 estimate_k=True,\n",
    "                                 standardize=False,\n",
    "                                 seed=1234)\n",
    "iris_kmeans.train(x=predictors,\n",
    "                  training_frame=train,\n",
    "                  validation_frame=valid)\n",
    "\n",
    "# Eval performance:\n",
    "perf = iris_kmeans.model_performance()\n",
    "\n",
    "#  Generate predictions on a validation set (if necessary):\n",
    "pred = iris_kmeans.predict(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e9286",
   "metadata": {},
   "source": [
    "#### Shut down H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3d2453",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown(prompt =False)"
   ]
  }
 ],
 "metadata": {
  "Rmd_chunk_options": {
   "author": "Your Name",
   "date": "2024-2-21",
   "output": "html_document",
   "title": "Unsupervised Model"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ],
    [
     "R",
     "ir",
     "",
     ""
    ],
    [
     "css",
     "css",
     "",
     ""
    ],
    [
     "Python3",
     "ir",
     "",
     ""
    ]
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
